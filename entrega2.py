# -*- coding: utf-8 -*-
"""Entrega2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cVTLTrTV9dm1iVuXCLtMDzpmdVQjWNcs

# Etapa 1 ‚Äì Pr√©-processamento dos Dados

*   Organizar os datasets de imagens e dados temporais coletados na Sprint 1.
*   Realizar tratamento dos dados, garantindo que estejam estruturados e prontos para serem usados no modelo.
*   Identificar padr√µes e sazonalidades na s√©rie temporal NDVI, explorando diferentes abordagens estat√≠sticas para entender varia√ß√µes de produtividade.

# Dados SatVeg

Dados obtidos do sensor SatVeg para a Fazenda S√£o Jos√©, Sorriso-MT
Coordenadas: -55.95520,-12.88229 | -55.95729,-12.88229

Bibliotecas necess√°rias
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
import numpy as np

"""Upload dos dados"""

# 1. Upload do arquivo
print("1. Carregamento dos dados do SatVeg")
from google.colab import files
uploaded = files.upload()

# Carregar o arquivo CSV
filename = list(uploaded.keys())[0]
df_satveg = pd.read_csv(filename, encoding='latin1', sep=';')

# Informa√ß√µes b√°sicas
print(f"Arquivo carregado: {filename}")
print(f"Dimens√µes: {df_satveg.shape[0]} linhas x {df_satveg.shape[1]} colunas")

## 2. Pr√©-processamento
print("\n2. Pr√©-processamento dos dados")

# Converter colunas data
if 'Data' in df_satveg.columns:
    df_satveg['Data'] = pd.to_datetime(df_satveg['Data'], format='%d/%m/%Y', errors='coerce')
    print("- Coluna 'Data' convertida para datetime")

# Criando colunas auxiliares
df_satveg['mes'] = df_satveg['Data'].dt.month
df_satveg['ano'] = df_satveg['Data'].dt.year

# Converter colunas num√©ricas
colunas_numericas = ["NDVI", "PreFiltro", "FlatBottom"]
print("- Convertendo colunas num√©ricas:")

for coluna in colunas_numericas:
    if coluna in df_satveg.columns:
        # Guardar tipo original para mostrar a mudan√ßa
        tipo_original = df_satveg[coluna].dtype

        # Converter de string com v√≠rgula para float
        if df_satveg[coluna].dtype == 'object':
            df_satveg[coluna] = df_satveg[coluna].str.replace(',', '.').astype(float)
            print(f"  Coluna '{coluna}' convertida: {tipo_original} ‚Üí {df_satveg[coluna].dtype}")
        else:
            print(f"  Coluna '{coluna}' j√° √© do tipo num√©rico: {df_satveg[coluna].dtype}")
    else:
        print(f"  Aten√ß√£o: Coluna '{coluna}' n√£o encontrada no DataFrame")

# Verificar valores nulos
nulos = df_satveg.isnull().sum()
print("- Valores nulos por coluna:")
print(nulos[nulos > 0] if any(nulos > 0) else "  N√£o h√° valores nulos")

df_satveg.head()

# Garante que 'Data' est√° como datetime
df_satveg['Data'] = pd.to_datetime(df_satveg['Data'], errors='coerce')

# Cria a coluna 'AnoMes' no formato 'YYYY-MM'
df_satveg['AnoMes'] = df_satveg['Data'].dt.to_period('M').astype(str)

if 'NDVI' in df_satveg.columns and 'AnoMes' in df_satveg.columns:
    # Para cada m√™s, calcula a m√©dia e atribui a todas as linhas daquele m√™s
    df_satveg['NDVI_media_mensal'] = df_satveg.groupby('AnoMes')['NDVI'].transform('mean')
    print("\nColuna 'NDVI_media_mensal' adicionada ao DataFrame!")
    print(df_satveg[['Data', 'NDVI', 'AnoMes', 'NDVI_media_mensal']].head())
else:
    print("\nN√£o foi poss√≠vel criar a m√©dia mensal do NDVI: Colunas necess√°rias n√£o encontradas.")

## 3. An√°lise Explorat√≥ria B√°sica
print("\n3. An√°lise Explorat√≥ria B√°sica")
print("- Primeiras linhas dos dados:")
display(df_satveg.head())

print("\n- Resumo estat√≠stico das colunas num√©ricas:")
display(df_satveg[colunas_numericas].describe())

## Visualiza√ß√£o de Dados
print("\n4. Visualiza√ß√£o de Dados")

# 4.1 Distribui√ß√µes (histogramas)
print(" Histograma - Distribui√ß√£o das vari√°veis:")
plt.figure(figsize=(15, 5))

for i, coluna in enumerate(colunas_numericas):
    plt.subplot(1, 3, i+1)
    sns.histplot(df_satveg[coluna], kde=True)
    plt.title(f'Distribui√ß√£o {coluna}')

plt.tight_layout()
plt.show()

# Serie temporal completa
plt.figure(figsize=(16,5))
plt.plot(df_satveg['Data'], df_satveg['NDVI'], marker='o', linestyle='-')
plt.title('NDVI ao longo do tempo')
plt.xlabel('Data')
plt.ylabel('NDVI')
plt.grid(True)
plt.tight_layout()
plt.show()

# Gr√°fico da m√©dia mensal (sazonalidade t√≠pica)
ndvi_media_mensal = df_satveg.groupby('mes')['NDVI'].mean()

plt.figure(figsize=(10,5))
plt.plot(ndvi_media_mensal.index, ndvi_media_mensal.values, marker='o')
plt.title('M√©dia Mensal do NDVI (Sazonalidade)')
plt.xlabel('M√™s')
plt.ylabel('NDVI m√©dio')
plt.xticks(range(1,13))
plt.grid(True)
plt.show()

# Boxplot do NDVI por m√™s
plt.figure(figsize=(12,6))
sns.boxplot(x='mes', y='NDVI', data=df_satveg)
plt.title('Distribui√ß√£o do NDVI por m√™s')
plt.xlabel('M√™s')
plt.ylabel('NDVI')
plt.grid(True)
plt.show()

# Heatmap Ano vs M√™s do NDVI
pivot = df_satveg.pivot_table(
    index='ano', columns='mes', values='NDVI', aggfunc='mean'
)

plt.figure(figsize=(14,6))
sns.heatmap(pivot, cmap='YlGn', annot=True, fmt=".2f")
plt.title('Heatmap NDVI: Ano x M√™s')
plt.xlabel('M√™s')
plt.ylabel('Ano')
plt.show()

#Gr√°fico de linhas sobrepostas por ano
plt.figure(figsize=(12,6))
for ano, grupo in df_satveg.groupby('ano'):
    plt.plot(grupo['mes'], grupo['NDVI'], marker='o', label=ano)

plt.title('NDVI Mensal: Comparativo entre anos')
plt.xlabel('M√™s')
plt.ylabel('NDVI')
plt.xticks(range(1, 13))
plt.legend(title="Ano")
plt.grid(True)
plt.tight_layout()
plt.show()

# Decomposi√ß√£o da s√©rie temporal (tend√™ncia + sazonalidade)

# Dados reagrupados mensalmente
ndvi_monthly = df_satveg.set_index('Data')['NDVI'].resample('M').mean().interpolate()

decomp = seasonal_decompose(ndvi_monthly, model='additive', period=12)
decomp.plot()
plt.suptitle('Decomposi√ß√£o s√©rie temporal do NDVI (mensal)')
plt.show()

# 4.3 Rela√ß√µes entre vari√°veis (dispers√£o)
print("\n Dispers√£o - Rela√ß√µes entre vari√°veis:")
plt.figure(figsize=(16, 5))

# Gr√°fico 1: NDVI vs PreFiltro
plt.subplot(1, 3, 1)
plt.scatter(df_satveg['NDVI'], df_satveg['PreFiltro'], alpha=0.6, color='green' )
plt.title('NDVI vs PreFiltro')
plt.xlabel('NDVI')
plt.ylabel('PreFiltro')
plt.grid(True, linestyle='--', alpha=0.7)

# Gr√°fico 2: NDVI vs FlatBottom
plt.subplot(1, 3, 2)
plt.scatter(df_satveg['NDVI'], df_satveg['FlatBottom'], alpha=0.7, color='blue')
plt.title('NDVI vs FlatBottom')
plt.xlabel('NDVI')
plt.ylabel('FlatBottom')
plt.grid(True, linestyle='--', alpha=0.7)

# Gr√°fico 3: PreFiltro vs FlatBottom
plt.subplot(1, 3, 3)
plt.scatter(df_satveg['PreFiltro'], df_satveg['FlatBottom'], alpha=0.7, color='orange')
plt.title('PreFiltro vs FlatBottom')
plt.xlabel('PreFiltro')
plt.ylabel('FlatBottom')
plt.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# 4.4 Compara√ß√£o de vari√°veis (boxplots)
print("\n Boxplots - Compara√ß√£o de distribui√ß√£o entre vari√°veis:")
plt.figure(figsize=(10, 6))
df_melt = df_satveg.melt(id_vars='Data', value_vars=colunas_numericas,
                  var_name='M√©trica', value_name='Valor')
sns.boxplot(x='M√©trica', y='Valor', data=df_melt)
plt.title('Distribui√ß√£o de valores das m√©tricas')
plt.grid(True)
plt.show()

# 4.5 Matriz de correla√ß√£o
print("\n Matriz de correla√ß√£o entre as m√©tricas:")
plt.figure(figsize=(10, 8))
corr = df_satveg[colunas_numericas].corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, annot=True, cmap='coolwarm', mask=mask, vmin=-1, vmax=1)
plt.title('Matriz de correla√ß√£o entre as m√©tricas')
plt.show()

## 5. Exporta√ß√£o dos dados processados
print("\n5. Exporta√ß√£o dos dados processados")
df_satveg.to_csv('satveg_processado.csv', index=False)
print("- Dados processados salvos como 'satveg_processado.csv'")

"""# Dados metereol√≥gicos INMET
Optamos por fazer download dos arquivos, pois via API ia ficar muito lento

Fonte de dados https://portal.inmet.gov.br/dadoshistoricos
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import glob

from google.colab import files
uploaded = files.upload()

# Padr√£o dos arquivos: Clima_Sorriso_*.csv
arquivos = sorted(glob.glob('Clima_Sorriso_*.csv'))  # lista todos os arquivos que batem com o padr√£o

print("Arquivos encontrados:", arquivos)

# L√™ todos os arquivos e empilha tudo em um s√≥ DataFrame
dfs = []
for arq in arquivos:
    print("Lendo:", arq)
    df_temp = pd.read_csv(arq, sep=";", encoding='latin1')
    dfs.append(df_temp)

# Junta tudo em um DataFrame s√≥
df_climatotal = pd.concat(dfs, ignore_index=True)

df_climatotal['DATA'] = pd.to_datetime(df_climatotal['DATA'], errors='coerce')

# Visualiza primeiras linhas da base consolidada
print("\nBase consolidada, preview:")
print(df_climatotal.head())
print(f"\nTotal de linhas lidas: {df_climatotal.shape[0]}")

"""Tratamento dos dados"""

# Validar os periodos do dataframe
df_climatotal['DATA'] = pd.to_datetime(df_climatotal['DATA'], format='%d/%m/%Y')

# 2. Encontrar a menor e maior data:
menor_data = df_climatotal['DATA'].min()
maior_data = df_climatotal['DATA'].max()

# 3. Mostrar
print("Menor data:", menor_data.strftime('%d/%m/%Y'))
print("Maior data:", maior_data.strftime('%d/%m/%Y'))

df_climatotal['RADIACAO'] = df_climatotal['RADIACAO'].replace(-9999, np.nan)
df_climatotal['TEMPERATURA'] = df_climatotal['TEMPERATURA'].replace(-9999, np.nan)

# Optamos por salvar o dataset unificado para facilitar o trabalho
df_climatotal.to_csv('clima_consolidados.csv', index=False)
print("Arquivo 'clima_consolidados.csv' salvo!")

# Valida√ß√£o do dataset consolidado
# Tamanho do dataset
print(f"Linhas: {df_climatotal.shape[0]}, Colunas: {df_climatotal.shape[1]}")

# Mostrar os primeiros registros
display(df_climatotal.head(10))

# Informa√ß√µes gerais das colunas
df_climatotal.info()

# Checar as primeiras datas (caso ainda precise revisar datas)
print("Menor data:", df_climatotal['DATA'].min())
print("Maior data:", df_climatotal['DATA'].max())

# Remover a coluna 'RADIACA' permanentemente do DataFrame
df_climatotal = df_climatotal.drop('RADIACA', axis=1)

df_climatotal.loc[df_climatotal['RADIACAO'] < 0, 'RADIACAO'] = np.nan

# Criando a coluna ANO_MES
df_climatotal['ANO_MES'] = df_climatotal['DATA'].dt.strftime('%Y-%m')


df_climatotal['DATA'] = pd.to_datetime(df_climatotal['DATA'], errors='coerce')
df_climatotal['ANO'] = df_climatotal['DATA'].dt.year.astype('Int64')
df_climatotal['MES'] = df_climatotal['DATA'].dt.month.astype('Int64')


print(df_climatotal[['DATA', 'ANO', 'MES', 'ANO_MES', 'RADIACAO', 'TEMPERATURA']].head())

#An√°lise de valores nulos, duplicados e convers√£o de tipo de coluna

# Nulos por coluna
print("\nValores nulos por coluna:")
print(df_climatotal.isnull().sum())

# % de nulos para priorizar tratamento
print("\nPorcentagem de nulos:")
print(df_climatotal.isnull().mean().sort_values(ascending=False)*100)

# Linhas duplicadas
print("\nQtde de linhas duplicadas:", df_climatotal.duplicated().sum())

print(df_climatotal.dtypes)

"""An√°lise explorat√≥ria"""

print(df_climatotal.describe())

"""Percebemos que a radia√ß√£o global esta muito alta"""

print('RADIACAO ‚Üí m√≠nimo:', df_climatotal['RADIACAO'].min(), 'm√°ximo:', df_climatotal['RADIACAO'].max())
print('TEMPERATURA ‚Üí m√≠nimo:', df_climatotal['TEMPERATURA'].min(), 'm√°ximo:', df_climatotal['TEMPERATURA'].max())
print('Primeiros 10 valores de RADIACAO:', df_climatotal['RADIACAO'].head(10).values)
print('Primeiros 10 valores de TEMPERATURA:', df_climatotal['TEMPERATURA'].head(10).values)

media = df_climatotal['RADIACAO'].mean()
std = df_climatotal['RADIACAO'].std()
print(f"M√©dia: {media:.2f}")
print(f"Desvio padr√£o: {std:.2f}")

limite_superior = media + 3*std
limite_inferior = media - 3*std

outliers = df_climatotal[(df_climatotal['RADIACAO'] > limite_superior) | (df_climatotal['RADIACAO'] < limite_inferior)]

print("Valores excessivos de RADIA√á√ÉO (outliers >3œÉ):")
print(outliers[['DATA', 'RADIACAO']])

print("10 maiores valores:")
print(df_climatotal[['DATA', 'RADIACAO']].sort_values('RADIACAO', ascending=False).head(10))

print("10 menores valores (diferentes de zero/nan):")
print(df_climatotal[['DATA', 'RADIACAO']][df_climatotal['RADIACAO'] > 0].sort_values('RADIACAO', ascending=True).head(10))

media = df_climatotal['RADIACAO'].mean()
std = df_climatotal['RADIACAO'].std()
limite_superior = media + 3*std
limite_inferior = media - 3*std

df_sem_outliers = df_climatotal[(df_climatotal['RADIACAO'] <= limite_superior) & (df_climatotal['RADIACAO'] >= limite_inferior)]

print('Original:', len(df_climatotal), 'Registros ap√≥s excluir outliers:', len(df_sem_outliers))

df_climatotal = df_climatotal.reset_index(drop=True)

print(df_climatotal.describe())

print("10 maiores valores:")
print(df_climatotal[['DATA', 'RADIACAO']].sort_values('RADIACAO', ascending=False).head(10))

print("10 menores valores (diferentes de zero/nan):")
print(df_climatotal[['DATA', 'RADIACAO']][df_climatotal['RADIACAO'] > 0].sort_values('RADIACAO', ascending=True).head(10))

# Gr√°fico de serie temporal
plt.figure(figsize=(15,5))
plt.plot(df_climatotal['DATA'], df_climatotal['RADIACAO'], color='orange')
plt.xlabel('DATA')
plt.ylabel('Radia√ß√£o (W/m¬≤)')
plt.title('Radia√ß√£o Solar ao Longo do Tempo')
plt.grid(True)
plt.tight_layout()
plt.show()

# Boxplot (total)
plt.figure(figsize=(6,5))
plt.boxplot(df_climatotal['RADIACAO'].dropna())
plt.ylabel('Radia√ß√£o (W/m¬≤)')
plt.title('Boxplot da Radia√ß√£o')
plt.show()

# Boxplot ano a ano
plt.figure(figsize=(10,6))
df_climatotal.boxplot(column='RADIACAO', by='ANO', grid=False)
plt.xlabel('Ano')
plt.ylabel('Radia√ß√£o (W/m¬≤)')
plt.title('Boxplot da Radia√ß√£o por Ano')
plt.suptitle('')
plt.tight_layout()
plt.show()

# Gr√°fico de dispers√£o - Radia√ß√£o x Ano
plt.figure(figsize=(10,6))
plt.scatter(df_climatotal['ANO'], df_climatotal['RADIACAO'], alpha=0.3, color='orange')
plt.xlabel('Ano')
plt.ylabel('Radia√ß√£o (W/m¬≤)')
plt.title('Dispers√£o de Radia√ß√£o por Ano')
plt.grid(True)
plt.tight_layout()
plt.show()

# Dispers√£o - M√©dias anuais
media_ano = df_climatotal.groupby('ANO')['RADIACAO'].mean().reset_index()

plt.figure(figsize=(10,6))
plt.scatter(media_ano['ANO'], media_ano['RADIACAO'], s=100, color='orange')
plt.plot(media_ano['ANO'], media_ano['RADIACAO'], color='orange', alpha=0.5)  # Linha para facilitar tend√™ncia
plt.xlabel('Ano')
plt.ylabel('M√©dia de Radia√ß√£o (W/m¬≤)')
plt.title('M√©dia Anual de Radia√ß√£o (Dispers√£o)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Gr√°fico de serie temporal
plt.figure(figsize=(15,5))
plt.plot(df_climatotal['DATA'], df_climatotal['TEMPERATURA'], color='green')
plt.xlabel('Data')
plt.ylabel('Temperatura (C)')
plt.title('Temperatura ao longo do tempo')
plt.grid(True)
plt.tight_layout()
plt.show()

# Boxplot (total)
plt.figure(figsize=(6,5))
plt.boxplot(df_climatotal['TEMPERATURA'].dropna())
plt.ylabel('Temperatura (C)')
plt.title('Boxplot da temperatura')
plt.show()

# Boxplot ano a ano
plt.figure(figsize=(10,6))
df_climatotal.boxplot(column='TEMPERATURA', by='ANO', grid=False)
plt.xlabel('Ano')
plt.ylabel('Temperatura (C)')
plt.title('Boxplot da temperatura por ano')
plt.suptitle('')
plt.tight_layout()
plt.show()

# Gr√°fico de dispers√£o - Radia√ß√£o x Ano
plt.figure(figsize=(10,6))
plt.scatter(df_climatotal['ANO'], df_climatotal['TEMPERATURA'], alpha=0.3, color='green')
plt.xlabel('Ano')
plt.ylabel('Temperatura (C)')
plt.title('Dispers√£o de temperatura por ano')
plt.grid(True)
plt.tight_layout()
plt.show()

# Dispers√£o - M√©dias anuais
media_ano = df_climatotal.groupby('ANO')['TEMPERATURA'].mean().reset_index()

plt.figure(figsize=(10,6))
plt.scatter(media_ano['ANO'], media_ano['TEMPERATURA'], s=100, color='green')
plt.plot(media_ano['ANO'], media_ano['TEMPERATURA'], color='green', alpha=0.5)  # Linha para facilitar tend√™ncia
plt.xlabel('Ano')
plt.ylabel('M√©dia de temperatura (C)')
plt.title('M√©dia anual de temperatura (Dispers√£o)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Exporta√ß√£o dos dados processados
print("\n5. Exporta√ß√£o dos dados processados")
df_climatotal.to_csv('climatotal.csv', index=False)
print("- Dados processados salvos como 'climatotal.csv'")

"""# Produ√ß√£o agr√≠cola üå±üåΩ
Fonte de dados CONAB, s√©rie hist√≥rica gr√£os https://portaldeinformacoes.conab.gov.br/downloads/arquivos/SerieHistoricaGraos.txt

> Adicionar aspas

Bibliotecas necess√°rias
"""

import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import altair as alt

"""Upload de arquivo"""

from google.colab import files
uploaded = files.upload()

# Check dos arquivos carregados
df = pd.read_csv("serie_historica_graos.csv", sep=";", encoding='latin1')
print("\nVisualiza√ß√£o correta do arquivo:")
print(df.head())

"""Tratamento dos dados

"""

# Filtrar apenas UF = MT (Mato Grosso)
df = df[df['uf'].str.strip().str.upper() == 'MT'].copy()

# Separar o ano_agricola em primeiro_ano e segundo_ano
def separa_anos(ano_agricola):
    if isinstance(ano_agricola, str) and '/' in ano_agricola:
        a1, a2 = ano_agricola.split('/')
        return a1.strip(), a2.strip()
    elif isinstance(ano_agricola, str):
        return ano_agricola.strip(), None
    else:
        return None, None

anos = df['ano_agricola'].apply(separa_anos)
df['primeiro_ano'] = anos.str[0]
df['segundo_ano'] = anos.str[1]

# Renomear colunas
df = df.rename(columns={
    'dsc_safra_previsao': 'safra',
    'area_plantada_mil_ha': 'area_plantada',
    'producao_mil_t': 'producao',
    'produtividade_mil_ha_mil_t': 'produtividade'
})

# Corrigir caracteres na coluna safra
def padroniza_safra(s):
    if not isinstance(s, str): return s
    s = s.upper().strip()
    # Corrige variantes de 1¬™ SAFRA
    s = re.sub(r'1[√ÇA¬™Aa]*\s*SAFRA', '1¬™ SAFRA', s)
    # Corrige variantes de 2¬™ SAFRA
    s = re.sub(r'2[√ÇA¬™Aa]*\s*SAFRA', '2¬™ SAFRA', s)
    # Corrige variantes de 3¬™ SAFRA
    s = re.sub(r'3[√ÇA¬™Aa]*\s*SAFRA', '3¬™ SAFRA', s)
    # Se tiver 'UNICA' transforma em '√öNICA' (opcional)
    s = re.sub(r'UNICA', '√öNICA', s)
    return s.title()

df['safra'] = df['safra'].apply(padroniza_safra)

# Filtrar somente produto MILHO (independente de espa√ßos mai√∫sculas ou acentua√ß√£o)
df = df[df['produto'].str.strip().str.upper() == 'MILHO']

# Selecionar colunas finais na ordem desejada
colunas = [
    'ano_agricola', 'primeiro_ano', 'segundo_ano', 'safra',
    'uf', 'produto', 'area_plantada', 'producao', 'produtividade'
]
df_milho = df[colunas].reset_index(drop=True)

# Remover valores nulos nas colunas principais
df_milho = df_milho.dropna(subset=[
    'ano_agricola', 'primeiro_ano', 'segundo_ano', 'safra',
    'area_plantada', 'producao', 'produtividade'
]).reset_index(drop=True)

# Print para conferir se sobraram nulos
print("\nValores nulos por coluna:")
print(df_milho.isnull().sum())

# Visualiza√ß√£o final
print(f"\nBase consolidada: {df_milho.shape[0]} linhas x {df_milho.shape[1]} colunas.")
print(df_milho.head())
print(f"O DataFrame tem {df_milho.shape[0]} linhas e {df_milho.shape[1]} colunas.")

"""Analise explorat√≥ria"""

print(df_milho.describe())

## Visualiza√ß√£o de Dados
print("\n4. Visualiza√ß√£o de Dados")

# Distribui√ß√µes (histogramas)
print(" Histograma - Distribui√ß√£o das vari√°veis:")
plt.figure(figsize=(15, 5))

# Define colunas_numericas aqui
colunas_numericas = ['area_plantada', 'producao', 'produtividade']

for i, coluna in enumerate(colunas_numericas):
    plt.subplot(1, 3, i+1)
    sns.histplot(df_milho[coluna], kde=True)
    plt.title(f'Distribui√ß√£o {coluna}')

plt.tight_layout()
plt.show()

# S√©ries temporais geral
print("\n Evolu√ß√£o temporal das m√©tricas:")
plt.figure(figsize=(14, 6))
for coluna in colunas_numericas:
    plt.plot(df_milho['primeiro_ano'], df_milho[coluna], label=coluna)
plt.title('Evolu√ß√£o temporal das m√©tricas')
plt.xlabel('Data')
plt.ylabel('Valor')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# S√©ries temporais de 2015 em diante
print("\n Evolu√ß√£o temporal das m√©tricas (2015 em diante):")
# Filtra para anos de 2015 para frente

# Converter 'primeiro_ano' para numerio
df_milho['primeiro_ano'] = pd.to_numeric(df_milho['primeiro_ano'], errors='coerce')
df_2015_em_diante = df_milho[df_milho['primeiro_ano'] >= 2015].copy()

print(f"Quantidade de linhas de 2015 em diante: {df_2015_em_diante.shape[0]}")

plt.figure(figsize=(14, 6))
for coluna in colunas_numericas:
    plt.plot(df_2015_em_diante['primeiro_ano'], df_2015_em_diante[coluna], marker='o', label=coluna)
plt.title('Evolu√ß√£o das m√©tricas (2015 em diante)')
plt.xlabel('Ano')
plt.ylabel('Valor')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Compara√ß√£o de vari√°veis (boxplots)
# Definir as vari√°veis num√©ricas de interesse
colunas_numericas = ['area_plantada', 'producao', 'produtividade']

# Boxplots em escalas separadas
print("\n Boxplots - Compara√ß√£o de distribui√ß√£o entre vari√°veis (escalas separadas):")
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.boxplot(y=df_milho['area_plantada'])
plt.title('√Årea Plantada')
plt.grid(True, linestyle='--', alpha=0.7)

plt.subplot(1, 3, 2)
sns.boxplot(y=df_milho['producao'])
plt.title('Produ√ß√£o')
plt.grid(True, linestyle='--', alpha=0.7)

plt.subplot(1, 3, 3)
sns.boxplot(y=df_milho['produtividade'])
plt.title('Produtividade')
plt.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# Boxplots normalizados para compara√ß√£o direta
print("\n Boxplots - Distribui√ß√£o normalizada para compara√ß√£o entre vari√°veis:")
plt.figure(figsize=(10, 6))

# Criar DataFrame para visualiza√ß√£o
df_melt = df_milho.melt(id_vars='primeiro_ano', value_vars=colunas_numericas,
                  var_name='M√©trica', value_name='Valor')

# Adicionar vers√£o normalizada das vari√°veis
df_norm = df_milho.copy()
for coluna in colunas_numericas:
    df_norm[coluna] = (df_milho[coluna] - df_milho[coluna].min()) / (df_milho[coluna].max() - df_milho[coluna].min())

df_melt_norm = df_norm.melt(id_vars='primeiro_ano', value_vars=colunas_numericas,
                  var_name='M√©trica', value_name='Valor Normalizado')

sns.boxplot(x='M√©trica', y='Valor Normalizado', data=df_melt_norm)
plt.title('Distribui√ß√£o normalizada das vari√°veis agr√≠colas')
plt.grid(True, linestyle='--', alpha=0.7)
plt.ylabel('Valores normalizados (0-1)')
plt.show()

# Rela√ß√µes entre vari√°veis (dispers√£o)
print("\n Dispers√£o - Rela√ß√µes entre vari√°veis:")
plt.figure(figsize=(16, 5))

# Gr√°fico 1: √Årea Plantada vs Produ√ß√£o
plt.subplot(1, 3, 1)
plt.scatter(df_milho['area_plantada'], df_milho['producao'], alpha=0.6, color='green')
plt.title('√Årea Plantada vs Produ√ß√£o')
plt.xlabel('√Årea Plantada (mil ha)')
plt.ylabel('Produ√ß√£o (mil t)')
plt.grid(True, linestyle='--', alpha=0.7)

# Gr√°fico 2: √Årea Plantada vs Produtividade
plt.subplot(1, 3, 2)
plt.scatter(df_milho['area_plantada'], df_milho['produtividade'], alpha=0.6, color='blue')
plt.title('√Årea Plantada vs Produtividade')
plt.xlabel('√Årea Plantada (mil ha)')
plt.ylabel('Produtividade (t/ha)')
plt.grid(True, linestyle='--', alpha=0.7)

# Gr√°fico 3: Produtividade vs Produ√ß√£o
plt.subplot(1, 3, 3)
plt.scatter(df_milho['produtividade'], df_milho['producao'], alpha=0.6, color='orange')
plt.title('Produtividade vs Produ√ß√£o')
plt.xlabel('Produtividade (t/ha)')
plt.ylabel('Produ√ß√£o (mil t)')
plt.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# Boxplots agrupados por safra
print("\n Boxplots - Evolu√ß√£o por safra:")
plt.figure(figsize=(15, 12))

plt.subplot(3, 1, 1)
sns.boxplot(x='primeiro_ano', y='area_plantada', data=df_milho)
plt.title('√Årea Plantada por Safra')
plt.grid(True, linestyle='--', alpha=0.7)
plt.xticks(rotation=45)

plt.subplot(3, 1, 2)
sns.boxplot(x='primeiro_ano', y='producao', data=df_milho)
plt.title('Produ√ß√£o por Safra')
plt.grid(True, linestyle='--', alpha=0.7)
plt.xticks(rotation=45)

plt.subplot(3, 1, 3)
sns.boxplot(x='primeiro_ano', y='produtividade', data=df_milho)
plt.title('Produtividade por Safra')
plt.grid(True, linestyle='--', alpha=0.7)
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Matriz de correla√ß√£o
print("\n Matriz de correla√ß√£o entre as m√©tricas:")
plt.figure(figsize=(10, 8))
corr = df_milho[colunas_numericas].corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, annot=True, cmap='coolwarm', mask=mask, vmin=-1, vmax=1)
plt.title('Matriz de correla√ß√£o entre as m√©tricas')
plt.show()

# Visualiza√ß√µes interativas com Altair
print("\n Visualiza√ß√£o interativa de s√©ries temporais:")

# Criar gr√°fico interativo para cada m√©trica
charts = []
for coluna in colunas_numericas:
    chart = alt.Chart(df_milho).mark_line().encode(
        x='primeiro_ano',
        y=alt.Y(coluna, title=coluna),
        tooltip=['primeiro_ano', coluna]
    ).properties(
        title=f'S√©rie Temporal - {coluna}',
        width=300,
        height=200
    )
    charts.append(chart)

# Exibir gr√°ficos em grid
display(alt.vconcat(*charts))

## 5. Exporta√ß√£o/Salvamento dos Dados Processados
print("\n5. Exporta√ß√£o dos Dados Processados")
df_milho.to_csv('milho.csv', index=False)
print("\n Dados filtrados salvos como 'milho.csv'")

"""# Etapa 2 ‚Äì Extra√ß√£o de Informa√ß√µes Relevantes:

*   Definir vari√°veis-chave a serem utilizadas no modelo, a partir da an√°lise
explorat√≥ria dos dados.
*   Analisar a rela√ß√£o entre NDVI e produtividade agr√≠cola, identificando os per√≠odos cr√≠ticos de crescimento da cultura.
*   Aplicar t√©cnicas de segmenta√ß√£o para destacar √°reas espec√≠ficas de cultivo na imagem de sat√©lite.

Montando o dataset unico

Definimos utilizar as variaveis abaixo pois apresentaram melhor... TERMINAR DE PREENCHER
"""

print (df_satveg.head())
#print (df_climatotal.head())

# Padroniza√ß√£o dos nomes das colunas
df_satveg.columns = df_satveg.columns.str.strip()
df_climatotal.columns = df_climatotal.columns.str.strip()
df_milho.columns = df_milho.columns.str.strip()

# Gerar NDVI m√©dio anual
# Calcular NDVI m√©dio anual
ndvi_medio_ano = (
    df_satveg.groupby('ano')['NDVI_media_mensal']
    .mean()
    .reset_index()
    .rename(columns={'NDVI_media_mensal': 'ndvi_medio_ano'})
)
print(ndvi_medio_ano.head())

print(df_climatotal.head())
print(df_climatotal.dtypes)

# Garantir tipos no clima e milho
# Tratando NA ou NaN antes
df_climatotal['ANO'] = pd.to_numeric(df_climatotal['ANO'], errors='coerce').astype('Int64')
df_milho['primeiro_ano'] = df_milho['primeiro_ano'].astype(int)
ndvi_medio_ano['ano'] = ndvi_medio_ano['ano'].astype(int)

print(df_climatotal.head())
print(df_milho.head())
print(ndvi_medio_ano.head())

# Filtrar anos a partir de 2015
anos_validos = set(
    df_milho[df_milho['primeiro_ano'] >= 2015]['primeiro_ano']
) & set(ndvi_medio_ano[ndvi_medio_ano['ano'] >= 2015]['ano']) & set(df_climatotal[df_climatotal['ANO'] >= 2015]['ANO'])

print(anos_validos)

milho_2015 = df_milho[df_milho['primeiro_ano'] >= 2015].copy()
ndvi_2015 = ndvi_medio_ano[ndvi_medio_ano['ano'] >= 2015].copy()
clima_2015 = df_climatotal[df_climatotal['ANO'] >= 2015].copy()

# Merges
# Merge Milho + NDVI anual
base = milho_2015.merge(ndvi_2015, left_on='primeiro_ano', right_on='ano', how='left')
#print(base.head())

#Merge resultado e clima
base = base.merge(clima_2015, left_on='primeiro_ano', right_on='ANO', how='left')
print(base.head())

# Interpola√ß√£o de clima

base = base.sort_values('primeiro_ano').reset_index(drop=True)

# # Criar as colunas 'temp_media_ciclo' e 'rad_media_ciclo' se n√£o existirem, preenchendo com NaN

if 'temp_media_ciclo' not in base.columns:
    base['temp_media_ciclo'] = np.nan
if 'rad_media_ciclo' not in base.columns:
    base['rad_media_ciclo'] = np.nan

# Flags de imputa√ß√£o para analisar depois quais foram preenchidos
base['imputado_temp'] = base['temp_media_ciclo'].isnull()
base['imputado_rad'] = base['rad_media_ciclo'].isnull()

# Interpola√ß√£o linear
base['temp_media_ciclo'] = base['temp_media_ciclo'].interpolate(method='linear', limit_direction='both')
base['rad_media_ciclo']  = base['rad_media_ciclo'].interpolate(method='linear', limit_direction='both')

# Validando resultados
print(base[['primeiro_ano', 'produtividade', 'ndvi_medio_ano',
            'temp_media_ciclo', 'rad_media_ciclo',
            'imputado_temp', 'imputado_rad']])

# Exporta dados processados
print("\n5. Exporta√ß√£o dos dados processados")
base.to_csv('base.csv', index=False)
print("- Dados processados salvos como 'base.csv'")

"""Analisar a rela√ß√£o entre NDVI e produtividade agr√≠cola, identificando os per√≠odos cr√≠ticos de crescimento da cultura.

"""

# Biblioteca necess√°rias
import pandas as pd
import matplotlib.pyplot as plt

# Upload do arquivo
base = pd.read_csv('base.csv')
print(base.head())

# Avaliar rela√ß√£o NDVI X produtividade (correla√ß√£o)
plt.figure(figsize=(10, 6))
plt.scatter(base['ndvi_medio_ano'], base['produtividade'])
plt.xlabel('NDVI m√©dio anual')
plt.ylabel('Produtividade (kg/ha)')
plt.title('Rela√ß√£o entre NDVI m√©dio anual e produtividade')
plt.show()

cor = base['ndvi_medio_ano'].corr(base['produtividade'])
print(f'Correla√ß√£o entre NDVI m√©dio anual e produtividade: {cor:.2f}')

# An√°lise de periodos criticos  (com NDVI mensal por ano)

# Criar tabela de NDVI mensal (pivoteamento)
tabela = df_satveg[df_satveg['ano'] >= 2015].pivot_table(index='ano', columns='mes', values='NDVI_media_mensal', aggfunc='mean')
tabela.columns = [f'ndvi_mes_{m:02d}' for m in tabela.columns]
tabela.reset_index(inplace=True)

# Juntar com produtividade
prod = base[['primeiro_ano', 'produtividade']].rename(columns={'primeiro_ano':'ano'})
analise = prod.merge(tabela, on='ano', how='left')

# Calcular correla√ß√£o de NDVI de cada m√™s X produtividade
correlacoes = {}
for col in analise.columns:
    if col.startswith('ndvi_mes_'):
        correlacoes[col] = analise[col].corr(analise['produtividade'])

correlacoes = pd.Series(correlacoes).sort_values(ascending=False)
print('- Correla√ß√£o de NDVI mensal com produtividade -')
print(correlacoes)

# Visualizar
correlacoes.plot(kind='bar', figsize=(10,4), title='Correla√ß√£o NDVI mensal vs. produtividade')
plt.ylabel('Correla√ß√£o')
plt.show()

plt.show()
print("\nJan apresentou a maior correla√ß√£o.\n")
print("Sugerindo que esse per√≠odo √© especialmente cr√≠tico para o sucesso produtivo da cultura analisada.")

# Janelas m√≥veis de NDVI m√©dio
for mes_inicio in range(1, 11):
    colunas = [f'ndvi_mes_{mes_inicio:02d}', f'ndvi_mes_{mes_inicio+1:02d}', f'ndvi_mes_{mes_inicio+2:02d}']
    if all(col in analise.columns for col in colunas):
        analise[f'janela_{mes_inicio:02d}_{mes_inicio+2:02d}'] = analise[colunas].mean(axis=1)

# Repetir a an√°lise de correla√ß√£o para as novas colunas
cor_janelas = {col: analise[col].corr(analise['produtividade'])
               for col in analise.columns if col.startswith('janela_')}
cor_janelas = pd.Series(cor_janelas).sort_values(ascending=False)
print('- Correla√ß√£o das janelas de NDVI com a produtividade -')
print(cor_janelas)

cor_janelas.plot(kind='bar', figsize=(10,4), title='Correla√ß√µes NDVI janelas vs. produtividade')
plt.ylabel('Correla√ß√£o')
plt.show()

plt.show()
print("\nJan/Mar apresentou a maior correla√ß√£o.\n")
print("Conclus√£o: Vigor da vegeta√ß√£o no in√≠cio do ciclo √© determinante para produtividade.")

"""Aplicar t√©cnicas de segmenta√ß√£o para destacar √°reas espec√≠ficas de cultivo na imagem de sat√©lite."""

# Instalar pacotes necess√°rios
!pip install earthengine-api geemap

# Importar bibliotecas
import ee
import geemap
import os
from google.colab import files

# Autenticar e inicializar o Earth Engine
ee.Authenticate()
ee.Initialize(project="pipoca-452815")

# Definir √°rea de interesse: Sorriso-MT
sorriso = ee.Geometry.Point(-55.71, -12.55).buffer(30000)  # 30km de buffer

# Definir per√≠odo de uma safra completa
start_date = '2023-10-01'  # In√≠cio do plantio da soja
end_date = '2024-04-30'    # Ap√≥s colheita da soja e poss√≠vel safrinha

# Obter cole√ß√£o Sentinel-2 com bandas espec√≠ficas
s2 = ee.ImageCollection('COPERNICUS/S2_SR') \
    .filterDate(start_date, end_date) \
# Obter cole√ß√£o Sentinel-2 com bandas espec√≠ficas
s2 = ee.ImageCollection('COPERNICUS/S2_SR') \
    .filterDate(start_date, end_date) \
    .filterBounds(sorriso) \
    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \
    .select(['B2', 'B3', 'B4', 'B8']) \
    .median()  # Mediana para remover nuvens

# Calcular NDVI
ndvi = s2.normalizedDifference(['B8', 'B4']).rename('NDVI')
s2 = s2.addBands(ndvi)

# Visualizar no mapa
Map = geemap.Map()
Map.centerObject(sorriso, 10)
Map.addLayer(s2, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000}, 'RGB Natural')
Map.addLayer(ndvi, {'min': 0, 'max': 0.8, 'palette': ['white', 'yellow', 'green']}, 'NDVI')
Map

# Montar o Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Definir pasta no Drive
drive_folder = "EarthEngine_Exports"  # Nome da pasta que ser√° criada no Drive

# Fun√ß√£o para exportar para o Drive
def export_to_drive(image, name, region, scale=30):
    task = ee.batch.Export.image.toDrive(
        image=image,
        description=name,
        folder=drive_folder,
        fileNamePrefix=name,
        region=region.getInfo()['coordinates'],
        scale=scale,
        maxPixels=1e9
    )
    task.start()
    print(f"Exportando {name} para o Google Drive...")
    return task

# Exportar imagens para o Drive
export_to_drive(
    s2.select(['B4', 'B3', 'B2']).divide(3000).multiply(255).byte(),
    "sorriso_rgb",
    sorriso,
    scale=30  # Resolu√ß√£o reduzida para tornar o arquivo menor
)

export_to_drive(
    ndvi,
    "sorriso_ndvi",
    sorriso,
    scale=30
)

print("\nImagens est√£o sendo exportadas para seu Google Drive.")
print("Este processo pode levar alguns minutos. As imagens estar√£o na pasta", drive_folder)
print("Voc√™ pode verificar o progresso em: https://code.earthengine.google.com/tasks")
print("Quando as tarefas estiverem conclu√≠das, voc√™ poder√° acessar os arquivos no seu Google Drive")

"""Upload das imagens do Drive e aplica√ß√£o das t√©cnicas de segmenta√ß√£o"""

# Montar o Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Importar bibliotecas necess√°rias
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from skimage import filters
from skimage.segmentation import watershed
from skimage.color import label2rgb
import rasterio

# Fun√ß√µes para carregar imagens
def load_rgb_image(path):
    with rasterio.open(path) as src:
        profile = src.profile
        # Ler as tr√™s bandas (R, G, B)
        rgb_raw = src.read()
        # Reorganizar para formato de visualiza√ß√£o (H, W, C)
        rgb = np.dstack((rgb_raw[0], rgb_raw[1], rgb_raw[2]))
        # Normalizar para visualiza√ß√£o
        rgb = np.clip(rgb/255.0, 0, 1)
        return rgb_raw, rgb, profile

def load_ndvi_image(path):
    with rasterio.open(path) as src:
        profile = src.profile
        # NDVI geralmente √© um √∫nico canal
        ndvi_raw = src.read(1)
        return ndvi_raw, profile

# Configure os caminhos para os arquivos no Drive
# Ajuste esses caminhos para onde seus arquivos est√£o armazenados
rgb_path = '/content/drive/MyDrive/Pipoca/sorriso_rgb.tif'
ndvi_path = '/content/drive/MyDrive/Pipoca/sorriso_ndvi.tif'

# Verificar se os arquivos existem antes de carregar
print("Carregando imagens...")
if os.path.exists(rgb_path) and os.path.exists(ndvi_path):
    rgb_raw, rgb, rgb_profile = load_rgb_image(rgb_path)
    ndvi_raw, ndvi_profile = load_ndvi_image(ndvi_path)
    print("Imagens carregadas com sucesso!")
else:
    print(f"Erro: Arquivo(s) n√£o encontrado(s).")
    print(f"Verifique se os caminhos est√£o corretos:")
    print(f"RGB: {rgb_path}")
    print(f"NDVI: {ndvi_path}")

    # Listar os arquivos dispon√≠veis na pasta EarthEngine_Exports
    export_dir = '/content/drive/MyDrive/EarthEngine_Exports/'
    if os.path.exists(export_dir):
        print("\nArquivos dispon√≠veis na pasta EarthEngine_Exports:")
        for file in os.listdir(export_dir):
            print(f"- {file}")
    else:
        print("\nPasta EarthEngine_Exports n√£o encontrada.")

# Visualizar as imagens
plt.figure(figsize=(15, 10))
plt.subplot(1, 2, 1)
plt.title('Imagem RGB')
plt.imshow(rgb)
plt.axis('off')
plt.subplot(1, 2, 2)
plt.title('Imagem NDVI')
plt.imshow(ndvi_raw, cmap='RdYlGn')
plt.colorbar(label='NDVI')
plt.axis('off')
plt.tight_layout()
plt.show()

# Imprimir algumas informa√ß√µes sobre as imagens
print(f"Dimens√µes da imagem RGB: {rgb.shape}")
print(f"Dimens√µes da imagem NDVI: {ndvi_raw.shape}")
print(f"Valor m√≠nimo NDVI: {np.min(ndvi_raw)}, Valor m√°ximo NDVI: {np.max(ndvi_raw)}")

# Segmenta√ß√£o baseada em limiar de NDVI
print("\n1. Aplicando segmenta√ß√£o por limiar de NDVI...")

# Definir limiares para diferentes classes de vegeta√ß√£o
ndvi_thresholds = {
    '√°gua': -1.0,
    'solo_nu': 0.2,
    'veg_esparsa': 0.4,
    'veg_densa': 0.6
}
# Criar mapa de segmenta√ß√£o baseado em NDVI
ndvi_segments = np.zeros_like(ndvi_raw, dtype=np.uint8)
ndvi_segments[(ndvi_raw > ndvi_thresholds['√°gua']) & (ndvi_raw <= ndvi_thresholds['solo_nu'])] = 1 # Solo nu
ndvi_segments[(ndvi_raw > ndvi_thresholds['solo_nu']) & (ndvi_raw <= ndvi_thresholds['veg_esparsa'])] = 2 # Vegeta√ß√£o esparsa
ndvi_segments[(ndvi_raw > ndvi_thresholds['veg_esparsa']) & (ndvi_raw <= ndvi_thresholds['veg_densa'])] = 3 # Vegeta√ß√£o moderada
ndvi_segments[ndvi_raw > ndvi_thresholds['veg_densa']] = 4 # Vegeta√ß√£o densa

# Visualizar segmenta√ß√£o por NDVI
plt.figure(figsize=(15, 10))
plt.subplot(1, 2, 1)
plt.title('Imagem NDVI Original')
plt.imshow(ndvi_raw, cmap='RdYlGn')
plt.colorbar(label='NDVI')
plt.axis('off')
plt.subplot(1, 2, 2)
plt.title('Segmenta√ß√£o por Limiares de NDVI')
cmap = plt.cm.get_cmap('viridis', 5) # 5 classes (incluindo 0)
im = plt.imshow(ndvi_segments, cmap=cmap)
cbar = plt.colorbar(im, ticks=[0, 1, 2, 3, 4])
cbar.set_ticklabels(['Sem dados', 'Solo nu', 'Vegeta√ß√£o esparsa', 'Vegeta√ß√£o moderada', 'Vegeta√ß√£o densa'])
plt.axis('off')
plt.tight_layout()
plt.show()

# M√©todos de segmenta√ß√£o (K-means e Watershed)
from sklearn.cluster import KMeans
from skimage import filters
from skimage.segmentation import watershed
from skimage.color import label2rgb

# Segmenta√ß√£o por clustering (K-MEANS)
print("\n2. Aplicando segmenta√ß√£o por K-means...")
# Preparar dados para clustering
# Reorganizar RGB para forma adequada
rgb_reshaped = rgb.reshape(rgb.shape[0], rgb.shape[1], 3)

# Empilhar RGB e NDVI
X = np.dstack([rgb_reshaped, ndvi_raw.reshape(ndvi_raw.shape[0], ndvi_raw.shape[1], 1)])

# Reshape para clustering
X_reshaped = X.reshape((-1, 4)) # Reshape para (n_pixels, n_features)

# Remover NaN ou valores inv√°lidos para o K-means
mask = ~np.isnan(X_reshaped).any(axis=1)
X_valid = X_reshaped[mask]

# Aplicar K-means (definir k=5 para diferentes tipos de cobertura do solo)
k = 5 # N√∫mero de clusters (ajuste conforme necess√°rio)
print(f"Aplicando K-means com {k} clusters em {X_valid.shape[0]} pixels v√°lidos...")
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_valid)

# Reconstruir imagem de segmenta√ß√£o
segmentation = np.zeros(X_reshaped.shape[0], dtype=np.uint8)
segmentation[mask] = labels + 1 # +1 para evitar 0
segmentation = segmentation.reshape(X.shape[:2])

# Visualizar segmenta√ß√£o por K-means
plt.figure(figsize=(15, 10))
plt.subplot(1, 2, 1)
plt.title('Imagem RGB Original')
plt.imshow(rgb)
plt.axis('off')
plt.subplot(1, 2, 2)
plt.title('Segmenta√ß√£o K-means (5 clusters)')
cmap = plt.cm.get_cmap('tab10', k+1) # +1 para incluir 0 (sem dados)
im = plt.imshow(segmentation, cmap=cmap)
plt.colorbar(im, ticks=range(k+1))
plt.axis('off')
plt.tight_layout()
plt.show()

# Segmenta~]ap de Watershed (baseada em gradientes)
print("\n3. Aplicando segmenta√ß√£o Watershed...")
# Criar uma imagem de gradiente a partir do NDVI para watershed
edges = filters.sobel(ndvi_raw)

# Usar watershed para segmentar
markers = np.zeros_like(ndvi_raw, dtype=np.int32)
# Definir marcadores baseados em NDVI
markers[ndvi_raw < 0.2] = 1 # Baixo NDVI (solo nu, √°gua)
markers[ndvi_raw > 0.6] = 2 # Alto NDVI (vegeta√ß√£o densa)

# Aplicar watershed
watershed_segments = watershed(edges, markers)

# Visualizar segmenta√ß√£o watershed
plt.figure(figsize=(15, 10))
plt.subplot(1, 2, 1)
plt.title('Imagem de Bordas (Gradientes)')
plt.imshow(edges, cmap='gray')
plt.axis('off')
plt.subplot(1, 2, 2)
plt.title('Segmenta√ß√£o Watershed')
overlay = label2rgb(watershed_segments, image=rgb, bg_label=0)
plt.imshow(overlay)
plt.axis('off')
plt.tight_layout()
plt.show()

"""# Etapa 3 ‚Äì Constru√ß√£o do Modelo de IA ü§ñ

*   Selecionar um modelo de aprendizado de m√°quina ou redes neurais para previs√£o da produtividade.
*   Treinar o modelo utilizando os dados hist√≥ricos e validar sua capacidade preditiva.
*   Ajustar hiperpar√¢metros e otimizar o desempenho do modelo.





"""

# Importar bibliotecas necess√°rias para modelagem
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns

# Extrair caracter√≠sticas das imagens segmentadas
def extract_features(rgb_image, ndvi_image, ndvi_segments, segments_kmeans, segments_watershed):
    # Criar dataframe para armazenar as caracter√≠sticas
    features = pd.DataFrame()

    # Estat√≠sticas b√°sicas do NDVI
    # Para cada segmento do K-means, calcular estat√≠sticas de NDVI
    for segment_id in np.unique(segments_kmeans):
        if segment_id == 0:
            continue  # Ignorar background

        mask = segments_kmeans == segment_id
        segment_ndvi = ndvi_image[mask]

        if len(segment_ndvi) > 0:
            features[f'ndvi_mean_segment_{segment_id}'] = [np.mean(segment_ndvi)]
            features[f'ndvi_std_segment_{segment_id}'] = [np.std(segment_ndvi)]
            features[f'ndvi_min_segment_{segment_id}'] = [np.min(segment_ndvi)]
            features[f'ndvi_max_segment_{segment_id}'] = [np.max(segment_ndvi)]
            features[f'area_segment_{segment_id}'] = [np.sum(mask)]

    # Caracter√≠sticas globais da imagem
    features['ndvi_mean_global'] = [np.mean(ndvi_image)]
    features['ndvi_std_global'] = [np.std(ndvi_image)]

    # Propor√ß√£o de cada classe de vegeta√ß√£o (baseada na segmenta√ß√£o por limiar de NDVI)
    total_pixels = ndvi_image.size
    for class_id in range(1, 5):  # Classes 1 a 4 (solo nu, veg esparsa, veg moderada, veg densa)
        class_count = np.sum(ndvi_segments == class_id)
        features[f'proportion_class_{class_id}'] = [class_count / total_pixels]

    # Caracter√≠sticas de textura (simplificadas)
    # Vari√¢ncia local como medida simples de textura
    from scipy.ndimage import generic_filter
    texture = generic_filter(ndvi_image, np.var, size=5)
    features['texture_mean'] = [np.mean(texture)]
    features['texture_std'] = [np.std(texture)]

    return features

# Aplicar a extra√ß√£o de caracter√≠sticas
print("Extraindo caracter√≠sticas das imagens...")
features_df = extract_features(rgb, ndvi_raw, ndvi_segments, segmentation, watershed_segments)
print(f"Caracter√≠sticas extra√≠das: {features_df.shape[1]}")
print(features_df.head())

# Prepara√ß√£o dos dados para treinamento (Dados CONAB)
# Carregando os dados
import pandas as pd

# Carregar dados de produtividade
produtividade = pd.read_csv('milho.csv')

print("Dados de produtividade carregados:")
print(f"N√∫mero de registros: {len(produtividade)}")
print(produtividade.head())

# Extrair caracter√≠sticas para cada imagem/parcela
all_features = pd.DataFrame()

# Para cada imagem/parcela no seu conjunto de dados
# Iterar sobre cada ano no conjunto de dados de produtividade
for ano in produtividade['primeiro_ano'].unique():
    # Filtrar os dados de produtividade para o ano atual
    data_ano = produtividade[produtividade['primeiro_ano'] == ano]

    # Carregar imagem e dados NDVI correspondentes
    # (ajuste os caminhos conforme necess√°rio)
    rgb_image, _, _ = load_rgb_image(rgb_path)  # Updated to use load_rgb_image
    ndvi_image, _ = load_ndvi_image(ndvi_path)  # Updated to use load_ndvi_image
    segments_kmeans = segmentation # Updated to use segmentation (replace with load segmentation if you have a different function)
    segments_watershed = watershed_segments # Updated to use watershed_segments (replace with load segmentation if you have a different function)
    ndvi_segments = ndvi_segments # Updated to use ndvi_segments (replace with load segmentation if you have a different function)

    # Extrair as caracter√≠sticas para o ano atual
    features = extract_features(rgb_image, ndvi_image, ndvi_segments, segments_kmeans, segments_watershed)

    # Adicionar o ano como identificador para jun√ß√£o posterior
    features['primeiro_ano'] = ano

    # Adicionar ao conjunto completo
    all_features = pd.concat([all_features, features], ignore_index=True)

# Juntar caracter√≠sticas com dados de produtividade
dataset = pd.merge(all_features, produtividade,
                  on=['primeiro_ano'],
                  how='inner')

print("\nConjunto de dados combinado:")
print(f"N√∫mero de registros: {len(dataset)}")
print(f"N√∫mero de caracter√≠sticas: {dataset.shape[1]}")
print(dataset.head())

# Verificar se h√° valores ausentes
missing_values = dataset.isnull().sum()
print("\nValores ausentes por coluna:")
print(missing_values[missing_values > 0])

# Tratar valores ausentes (se necess√°rio)
dataset = dataset.dropna()  # Ou use dataset.fillna() para preencher

# Dividir em caracter√≠sticas (X) e alvo (y)
X = dataset.drop(['productivity', 'parcel_id', 'date', 'image_path', 'ndvi_path',
                 'segmentation_path', 'watershed_path', 'ndvi_segments_path'], axis=1)
y = dataset['productivity']

# Dividir em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("\nDados divididos em conjuntos de treinamento e teste:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Normalizar os dados
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nDados normalizados e prontos para treinamento.")
