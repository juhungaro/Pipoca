# -*- coding: utf-8 -*-
"""Entrega2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cVTLTrTV9dm1iVuXCLtMDzpmdVQjWNcs

# Etapa 1 – Pré-processamento dos Dados

*   Organizar os datasets de imagens e dados temporais coletados na Sprint 1.
*   Realizar tratamento dos dados, garantindo que estejam estruturados e prontos para serem usados no modelo.
*   Identificar padrões e sazonalidades na série temporal NDVI, explorando diferentes abordagens estatísticas para entender variações de produtividade.

# Dados SatVeg

Dados obtidos do sensor SatVeg para a Fazenda São José, Sorriso-MT
Coordenadas: -55.95520,-12.88229 | -55.95729,-12.88229

Bibliotecas necessárias
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
import numpy as np

"""Upload dos dados"""

# 1. Upload do arquivo
print("1. Carregamento dos dados do SatVeg")
from google.colab import files
uploaded = files.upload()

# Carregar o arquivo CSV
filename = list(uploaded.keys())[0]
df_satveg = pd.read_csv(filename, encoding='latin1', sep=';')

# Informações básicas
print(f"Arquivo carregado: {filename}")
print(f"Dimensões: {df_satveg.shape[0]} linhas x {df_satveg.shape[1]} colunas")

## 2. Pré-processamento
print("\n2. Pré-processamento dos dados")

# Converter colunas data
if 'Data' in df_satveg.columns:
    df_satveg['Data'] = pd.to_datetime(df_satveg['Data'], format='%d/%m/%Y', errors='coerce')
    print("- Coluna 'Data' convertida para datetime")

# Criando colunas auxiliares
df_satveg['mes'] = df_satveg['Data'].dt.month
df_satveg['ano'] = df_satveg['Data'].dt.year

# Converter colunas numéricas
colunas_numericas = ["NDVI", "PreFiltro", "FlatBottom"]
print("- Convertendo colunas numéricas:")

for coluna in colunas_numericas:
    if coluna in df_satveg.columns:
        # Guardar tipo original para mostrar a mudança
        tipo_original = df_satveg[coluna].dtype

        # Converter de string com vírgula para float
        if df_satveg[coluna].dtype == 'object':
            df_satveg[coluna] = df_satveg[coluna].str.replace(',', '.').astype(float)
            print(f"  Coluna '{coluna}' convertida: {tipo_original} → {df_satveg[coluna].dtype}")
        else:
            print(f"  Coluna '{coluna}' já é do tipo numérico: {df_satveg[coluna].dtype}")
    else:
        print(f"  Atenção: Coluna '{coluna}' não encontrada no DataFrame")

# Verificar valores nulos
nulos = df_satveg.isnull().sum()
print("- Valores nulos por coluna:")
print(nulos[nulos > 0] if any(nulos > 0) else "  Não há valores nulos")

df_satveg.head()

# Garante que 'Data' está como datetime
df_satveg['Data'] = pd.to_datetime(df_satveg['Data'], errors='coerce')

# Cria a coluna 'AnoMes' no formato 'YYYY-MM'
df_satveg['AnoMes'] = df_satveg['Data'].dt.to_period('M').astype(str)

if 'NDVI' in df_satveg.columns and 'AnoMes' in df_satveg.columns:
    # Para cada mês, calcula a média e atribui a todas as linhas daquele mês
    df_satveg['NDVI_media_mensal'] = df_satveg.groupby('AnoMes')['NDVI'].transform('mean')
    print("\nColuna 'NDVI_media_mensal' adicionada ao DataFrame!")
    print(df_satveg[['Data', 'NDVI', 'AnoMes', 'NDVI_media_mensal']].head())
else:
    print("\nNão foi possível criar a média mensal do NDVI: Colunas necessárias não encontradas.")

## 3. Análise Exploratória Básica
print("\n3. Análise Exploratória Básica")
print("- Primeiras linhas dos dados:")
display(df_satveg.head())

print("\n- Resumo estatístico das colunas numéricas:")
display(df_satveg[colunas_numericas].describe())

## Visualização de Dados
print("\n4. Visualização de Dados")

# 4.1 Distribuições (histogramas)
print(" Histograma - Distribuição das variáveis:")
plt.figure(figsize=(15, 5))

for i, coluna in enumerate(colunas_numericas):
    plt.subplot(1, 3, i+1)
    sns.histplot(df_satveg[coluna], kde=True)
    plt.title(f'Distribuição {coluna}')

plt.tight_layout()
plt.show()

# Serie temporal completa
plt.figure(figsize=(16,5))
plt.plot(df_satveg['Data'], df_satveg['NDVI'], marker='o', linestyle='-')
plt.title('NDVI ao longo do tempo')
plt.xlabel('Data')
plt.ylabel('NDVI')
plt.grid(True)
plt.tight_layout()
plt.show()

# Gráfico da média mensal (sazonalidade típica)
ndvi_media_mensal = df_satveg.groupby('mes')['NDVI'].mean()

plt.figure(figsize=(10,5))
plt.plot(ndvi_media_mensal.index, ndvi_media_mensal.values, marker='o')
plt.title('Média Mensal do NDVI (Sazonalidade)')
plt.xlabel('Mês')
plt.ylabel('NDVI médio')
plt.xticks(range(1,13))
plt.grid(True)
plt.show()

# Boxplot do NDVI por mês
plt.figure(figsize=(12,6))
sns.boxplot(x='mes', y='NDVI', data=df_satveg)
plt.title('Distribuição do NDVI por mês')
plt.xlabel('Mês')
plt.ylabel('NDVI')
plt.grid(True)
plt.show()

# Heatmap Ano vs Mês do NDVI
pivot = df_satveg.pivot_table(
    index='ano', columns='mes', values='NDVI', aggfunc='mean'
)

plt.figure(figsize=(14,6))
sns.heatmap(pivot, cmap='YlGn', annot=True, fmt=".2f")
plt.title('Heatmap NDVI: Ano x Mês')
plt.xlabel('Mês')
plt.ylabel('Ano')
plt.show()

#Gráfico de linhas sobrepostas por ano
plt.figure(figsize=(12,6))
for ano, grupo in df_satveg.groupby('ano'):
    plt.plot(grupo['mes'], grupo['NDVI'], marker='o', label=ano)

plt.title('NDVI Mensal: Comparativo entre anos')
plt.xlabel('Mês')
plt.ylabel('NDVI')
plt.xticks(range(1, 13))
plt.legend(title="Ano")
plt.grid(True)
plt.tight_layout()
plt.show()

# Decomposição da série temporal (tendência + sazonalidade)

# Dados reagrupados mensalmente
ndvi_monthly = df_satveg.set_index('Data')['NDVI'].resample('M').mean().interpolate()

decomp = seasonal_decompose(ndvi_monthly, model='additive', period=12)
decomp.plot()
plt.suptitle('Decomposição série temporal do NDVI (mensal)')
plt.show()

# 4.3 Relações entre variáveis (dispersão)
print("\n Dispersão - Relações entre variáveis:")
plt.figure(figsize=(16, 5))

# Gráfico 1: NDVI vs PreFiltro
plt.subplot(1, 3, 1)
plt.scatter(df_satveg['NDVI'], df_satveg['PreFiltro'], alpha=0.6, color='green' )
plt.title('NDVI vs PreFiltro')
plt.xlabel('NDVI')
plt.ylabel('PreFiltro')
plt.grid(True, linestyle='--', alpha=0.7)

# Gráfico 2: NDVI vs FlatBottom
plt.subplot(1, 3, 2)
plt.scatter(df_satveg['NDVI'], df_satveg['FlatBottom'], alpha=0.7, color='blue')
plt.title('NDVI vs FlatBottom')
plt.xlabel('NDVI')
plt.ylabel('FlatBottom')
plt.grid(True, linestyle='--', alpha=0.7)

# Gráfico 3: PreFiltro vs FlatBottom
plt.subplot(1, 3, 3)
plt.scatter(df_satveg['PreFiltro'], df_satveg['FlatBottom'], alpha=0.7, color='orange')
plt.title('PreFiltro vs FlatBottom')
plt.xlabel('PreFiltro')
plt.ylabel('FlatBottom')
plt.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# 4.4 Comparação de variáveis (boxplots)
print("\n Boxplots - Comparação de distribuição entre variáveis:")
plt.figure(figsize=(10, 6))
df_melt = df_satveg.melt(id_vars='Data', value_vars=colunas_numericas,
                  var_name='Métrica', value_name='Valor')
sns.boxplot(x='Métrica', y='Valor', data=df_melt)
plt.title('Distribuição de valores das métricas')
plt.grid(True)
plt.show()

# 4.5 Matriz de correlação
print("\n Matriz de correlação entre as métricas:")
plt.figure(figsize=(10, 8))
corr = df_satveg[colunas_numericas].corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, annot=True, cmap='coolwarm', mask=mask, vmin=-1, vmax=1)
plt.title('Matriz de correlação entre as métricas')
plt.show()

## 5. Exportação dos dados processados
print("\n5. Exportação dos dados processados")
df_satveg.to_csv('satveg_processado.csv', index=False)
print("- Dados processados salvos como 'satveg_processado.csv'")

"""# Dados metereológicos INMET
Optamos por fazer download dos arquivos, pois via API ia ficar muito lento

Fonte de dados https://portal.inmet.gov.br/dadoshistoricos
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import glob

from google.colab import files
uploaded = files.upload()

# Padrão dos arquivos: Clima_Sorriso_*.csv
arquivos = sorted(glob.glob('Clima_Sorriso_*.csv'))  # lista todos os arquivos que batem com o padrão

print("Arquivos encontrados:", arquivos)

# Lê todos os arquivos e empilha tudo em um só DataFrame
dfs = []
for arq in arquivos:
    print("Lendo:", arq)
    df_temp = pd.read_csv(arq, sep=";", encoding='latin1')
    dfs.append(df_temp)

# Junta tudo em um DataFrame só
df_climatotal = pd.concat(dfs, ignore_index=True)

df_climatotal['DATA'] = pd.to_datetime(df_climatotal['DATA'], errors='coerce')

# Visualiza primeiras linhas da base consolidada
print("\nBase consolidada, preview:")
print(df_climatotal.head())
print(f"\nTotal de linhas lidas: {df_climatotal.shape[0]}")

"""Tratamento dos dados"""

# Validar os periodos do dataframe
df_climatotal['DATA'] = pd.to_datetime(df_climatotal['DATA'], format='%d/%m/%Y')

# 2. Encontrar a menor e maior data:
menor_data = df_climatotal['DATA'].min()
maior_data = df_climatotal['DATA'].max()

# 3. Mostrar
print("Menor data:", menor_data.strftime('%d/%m/%Y'))
print("Maior data:", maior_data.strftime('%d/%m/%Y'))

df_climatotal['RADIACAO'] = df_climatotal['RADIACAO'].replace(-9999, np.nan)
df_climatotal['TEMPERATURA'] = df_climatotal['TEMPERATURA'].replace(-9999, np.nan)

# Optamos por salvar o dataset unificado para facilitar o trabalho
df_climatotal.to_csv('clima_consolidados.csv', index=False)
print("Arquivo 'clima_consolidados.csv' salvo!")

# Validação do dataset consolidado
# Tamanho do dataset
print(f"Linhas: {df_climatotal.shape[0]}, Colunas: {df_climatotal.shape[1]}")

# Mostrar os primeiros registros
display(df_climatotal.head(10))

# Informações gerais das colunas
df_climatotal.info()

# Checar as primeiras datas (caso ainda precise revisar datas)
print("Menor data:", df_climatotal['DATA'].min())
print("Maior data:", df_climatotal['DATA'].max())

# Remover a coluna 'RADIACA' permanentemente do DataFrame
df_climatotal = df_climatotal.drop('RADIACA', axis=1)

df_climatotal.loc[df_climatotal['RADIACAO'] < 0, 'RADIACAO'] = np.nan

# Criando a coluna ANO_MES
df_climatotal['ANO_MES'] = df_climatotal['DATA'].dt.strftime('%Y-%m')


df_climatotal['DATA'] = pd.to_datetime(df_climatotal['DATA'], errors='coerce')
df_climatotal['ANO'] = df_climatotal['DATA'].dt.year.astype('Int64')
df_climatotal['MES'] = df_climatotal['DATA'].dt.month.astype('Int64')


print(df_climatotal[['DATA', 'ANO', 'MES', 'ANO_MES', 'RADIACAO', 'TEMPERATURA']].head())

#Análise de valores nulos, duplicados e conversão de tipo de coluna

# Nulos por coluna
print("\nValores nulos por coluna:")
print(df_climatotal.isnull().sum())

# % de nulos para priorizar tratamento
print("\nPorcentagem de nulos:")
print(df_climatotal.isnull().mean().sort_values(ascending=False)*100)

# Linhas duplicadas
print("\nQtde de linhas duplicadas:", df_climatotal.duplicated().sum())

print(df_climatotal.dtypes)

"""Análise exploratória"""

print(df_climatotal.describe())

"""Percebemos que a radiação global esta muito alta"""

print('RADIACAO → mínimo:', df_climatotal['RADIACAO'].min(), 'máximo:', df_climatotal['RADIACAO'].max())
print('TEMPERATURA → mínimo:', df_climatotal['TEMPERATURA'].min(), 'máximo:', df_climatotal['TEMPERATURA'].max())
print('Primeiros 10 valores de RADIACAO:', df_climatotal['RADIACAO'].head(10).values)
print('Primeiros 10 valores de TEMPERATURA:', df_climatotal['TEMPERATURA'].head(10).values)

media = df_climatotal['RADIACAO'].mean()
std = df_climatotal['RADIACAO'].std()
print(f"Média: {media:.2f}")
print(f"Desvio padrão: {std:.2f}")

limite_superior = media + 3*std
limite_inferior = media - 3*std

outliers = df_climatotal[(df_climatotal['RADIACAO'] > limite_superior) | (df_climatotal['RADIACAO'] < limite_inferior)]

print("Valores excessivos de RADIAÇÃO (outliers >3σ):")
print(outliers[['DATA', 'RADIACAO']])

print("10 maiores valores:")
print(df_climatotal[['DATA', 'RADIACAO']].sort_values('RADIACAO', ascending=False).head(10))

print("10 menores valores (diferentes de zero/nan):")
print(df_climatotal[['DATA', 'RADIACAO']][df_climatotal['RADIACAO'] > 0].sort_values('RADIACAO', ascending=True).head(10))

media = df_climatotal['RADIACAO'].mean()
std = df_climatotal['RADIACAO'].std()
limite_superior = media + 3*std
limite_inferior = media - 3*std

df_sem_outliers = df_climatotal[(df_climatotal['RADIACAO'] <= limite_superior) & (df_climatotal['RADIACAO'] >= limite_inferior)]

print('Original:', len(df_climatotal), 'Registros após excluir outliers:', len(df_sem_outliers))

df_climatotal = df_climatotal.reset_index(drop=True)

print(df_climatotal.describe())

print("10 maiores valores:")
print(df_climatotal[['DATA', 'RADIACAO']].sort_values('RADIACAO', ascending=False).head(10))

print("10 menores valores (diferentes de zero/nan):")
print(df_climatotal[['DATA', 'RADIACAO']][df_climatotal['RADIACAO'] > 0].sort_values('RADIACAO', ascending=True).head(10))

# Gráfico de serie temporal
plt.figure(figsize=(15,5))
plt.plot(df_climatotal['DATA'], df_climatotal['RADIACAO'], color='orange')
plt.xlabel('DATA')
plt.ylabel('Radiação (W/m²)')
plt.title('Radiação Solar ao Longo do Tempo')
plt.grid(True)
plt.tight_layout()
plt.show()

# Boxplot (total)
plt.figure(figsize=(6,5))
plt.boxplot(df_climatotal['RADIACAO'].dropna())
plt.ylabel('Radiação (W/m²)')
plt.title('Boxplot da Radiação')
plt.show()

# Boxplot ano a ano
plt.figure(figsize=(10,6))
df_climatotal.boxplot(column='RADIACAO', by='ANO', grid=False)
plt.xlabel('Ano')
plt.ylabel('Radiação (W/m²)')
plt.title('Boxplot da Radiação por Ano')
plt.suptitle('')
plt.tight_layout()
plt.show()

# Gráfico de dispersão - Radiação x Ano
plt.figure(figsize=(10,6))
plt.scatter(df_climatotal['ANO'], df_climatotal['RADIACAO'], alpha=0.3, color='orange')
plt.xlabel('Ano')
plt.ylabel('Radiação (W/m²)')
plt.title('Dispersão de Radiação por Ano')
plt.grid(True)
plt.tight_layout()
plt.show()

# Dispersão - Médias anuais
media_ano = df_climatotal.groupby('ANO')['RADIACAO'].mean().reset_index()

plt.figure(figsize=(10,6))
plt.scatter(media_ano['ANO'], media_ano['RADIACAO'], s=100, color='orange')
plt.plot(media_ano['ANO'], media_ano['RADIACAO'], color='orange', alpha=0.5)  # Linha para facilitar tendência
plt.xlabel('Ano')
plt.ylabel('Média de Radiação (W/m²)')
plt.title('Média Anual de Radiação (Dispersão)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Gráfico de serie temporal
plt.figure(figsize=(15,5))
plt.plot(df_climatotal['DATA'], df_climatotal['TEMPERATURA'], color='green')
plt.xlabel('Data')
plt.ylabel('Temperatura (C)')
plt.title('Temperatura ao longo do tempo')
plt.grid(True)
plt.tight_layout()
plt.show()

# Boxplot (total)
plt.figure(figsize=(6,5))
plt.boxplot(df_climatotal['TEMPERATURA'].dropna())
plt.ylabel('Temperatura (C)')
plt.title('Boxplot da temperatura')
plt.show()

# Boxplot ano a ano
plt.figure(figsize=(10,6))
df_climatotal.boxplot(column='TEMPERATURA', by='ANO', grid=False)
plt.xlabel('Ano')
plt.ylabel('Temperatura (C)')
plt.title('Boxplot da temperatura por ano')
plt.suptitle('')
plt.tight_layout()
plt.show()

# Gráfico de dispersão - Radiação x Ano
plt.figure(figsize=(10,6))
plt.scatter(df_climatotal['ANO'], df_climatotal['TEMPERATURA'], alpha=0.3, color='green')
plt.xlabel('Ano')
plt.ylabel('Temperatura (C)')
plt.title('Dispersão de temperatura por ano')
plt.grid(True)
plt.tight_layout()
plt.show()

# Dispersão - Médias anuais
media_ano = df_climatotal.groupby('ANO')['TEMPERATURA'].mean().reset_index()

plt.figure(figsize=(10,6))
plt.scatter(media_ano['ANO'], media_ano['TEMPERATURA'], s=100, color='green')
plt.plot(media_ano['ANO'], media_ano['TEMPERATURA'], color='green', alpha=0.5)  # Linha para facilitar tendência
plt.xlabel('Ano')
plt.ylabel('Média de temperatura (C)')
plt.title('Média anual de temperatura (Dispersão)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Exportação dos dados processados
print("\n5. Exportação dos dados processados")
df_climatotal.to_csv('climatotal.csv', index=False)
print("- Dados processados salvos como 'climatotal.csv'")

"""# Produção agrícola 🌱🌽
Fonte de dados CONAB, série histórica grãos https://portaldeinformacoes.conab.gov.br/downloads/arquivos/SerieHistoricaGraos.txt

> Adicionar aspas

Bibliotecas necessárias
"""

import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import altair as alt

"""Upload de arquivo"""

from google.colab import files
uploaded = files.upload()

# Check dos arquivos carregados
df = pd.read_csv("serie_historica_graos.csv", sep=";", encoding='latin1')
print("\nVisualização correta do arquivo:")
print(df.head())

"""Tratamento dos dados

"""

# Filtrar apenas UF = MT (Mato Grosso)
df = df[df['uf'].str.strip().str.upper() == 'MT'].copy()

# Separar o ano_agricola em primeiro_ano e segundo_ano
def separa_anos(ano_agricola):
    if isinstance(ano_agricola, str) and '/' in ano_agricola:
        a1, a2 = ano_agricola.split('/')
        return a1.strip(), a2.strip()
    elif isinstance(ano_agricola, str):
        return ano_agricola.strip(), None
    else:
        return None, None

anos = df['ano_agricola'].apply(separa_anos)
df['primeiro_ano'] = anos.str[0]
df['segundo_ano'] = anos.str[1]

# Renomear colunas
df = df.rename(columns={
    'dsc_safra_previsao': 'safra',
    'area_plantada_mil_ha': 'area_plantada',
    'producao_mil_t': 'producao',
    'produtividade_mil_ha_mil_t': 'produtividade'
})

# Corrigir caracteres na coluna safra
def padroniza_safra(s):
    if not isinstance(s, str): return s
    s = s.upper().strip()
    # Corrige variantes de 1ª SAFRA
    s = re.sub(r'1[ÂAªAa]*\s*SAFRA', '1ª SAFRA', s)
    # Corrige variantes de 2ª SAFRA
    s = re.sub(r'2[ÂAªAa]*\s*SAFRA', '2ª SAFRA', s)
    # Corrige variantes de 3ª SAFRA
    s = re.sub(r'3[ÂAªAa]*\s*SAFRA', '3ª SAFRA', s)
    # Se tiver 'UNICA' transforma em 'ÚNICA' (opcional)
    s = re.sub(r'UNICA', 'ÚNICA', s)
    return s.title()

df['safra'] = df['safra'].apply(padroniza_safra)

# Filtrar somente produto MILHO (independente de espaços maiúsculas ou acentuação)
df = df[df['produto'].str.strip().str.upper() == 'MILHO']

# Selecionar colunas finais na ordem desejada
colunas = [
    'ano_agricola', 'primeiro_ano', 'segundo_ano', 'safra',
    'uf', 'produto', 'area_plantada', 'producao', 'produtividade'
]
df_milho = df[colunas].reset_index(drop=True)

# Remover valores nulos nas colunas principais
df_milho = df_milho.dropna(subset=[
    'ano_agricola', 'primeiro_ano', 'segundo_ano', 'safra',
    'area_plantada', 'producao', 'produtividade'
]).reset_index(drop=True)

# Print para conferir se sobraram nulos
print("\nValores nulos por coluna:")
print(df_milho.isnull().sum())

# Visualização final
print(f"\nBase consolidada: {df_milho.shape[0]} linhas x {df_milho.shape[1]} colunas.")
print(df_milho.head())
print(f"O DataFrame tem {df_milho.shape[0]} linhas e {df_milho.shape[1]} colunas.")

"""Analise exploratória"""

print(df_milho.describe())

## Visualização de Dados
print("\n4. Visualização de Dados")

# Distribuições (histogramas)
print(" Histograma - Distribuição das variáveis:")
plt.figure(figsize=(15, 5))

# Define colunas_numericas aqui
colunas_numericas = ['area_plantada', 'producao', 'produtividade']

for i, coluna in enumerate(colunas_numericas):
    plt.subplot(1, 3, i+1)
    sns.histplot(df_milho[coluna], kde=True)
    plt.title(f'Distribuição {coluna}')

plt.tight_layout()
plt.show()

# Séries temporais geral
print("\n Evolução temporal das métricas:")
plt.figure(figsize=(14, 6))
for coluna in colunas_numericas:
    plt.plot(df_milho['primeiro_ano'], df_milho[coluna], label=coluna)
plt.title('Evolução temporal das métricas')
plt.xlabel('Data')
plt.ylabel('Valor')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Séries temporais de 2015 em diante
print("\n Evolução temporal das métricas (2015 em diante):")
# Filtra para anos de 2015 para frente

# Converter 'primeiro_ano' para numerio
df_milho['primeiro_ano'] = pd.to_numeric(df_milho['primeiro_ano'], errors='coerce')
df_2015_em_diante = df_milho[df_milho['primeiro_ano'] >= 2015].copy()

print(f"Quantidade de linhas de 2015 em diante: {df_2015_em_diante.shape[0]}")

plt.figure(figsize=(14, 6))
for coluna in colunas_numericas:
    plt.plot(df_2015_em_diante['primeiro_ano'], df_2015_em_diante[coluna], marker='o', label=coluna)
plt.title('Evolução das métricas (2015 em diante)')
plt.xlabel('Ano')
plt.ylabel('Valor')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Comparação de variáveis (boxplots)
# Definir as variáveis numéricas de interesse
colunas_numericas = ['area_plantada', 'producao', 'produtividade']

# Boxplots em escalas separadas
print("\n Boxplots - Comparação de distribuição entre variáveis (escalas separadas):")
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.boxplot(y=df_milho['area_plantada'])
plt.title('Área Plantada')
plt.grid(True, linestyle='--', alpha=0.7)

plt.subplot(1, 3, 2)
sns.boxplot(y=df_milho['producao'])
plt.title('Produção')
plt.grid(True, linestyle='--', alpha=0.7)

plt.subplot(1, 3, 3)
sns.boxplot(y=df_milho['produtividade'])
plt.title('Produtividade')
plt.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# Boxplots normalizados para comparação direta
print("\n Boxplots - Distribuição normalizada para comparação entre variáveis:")
plt.figure(figsize=(10, 6))

# Criar DataFrame para visualização
df_melt = df_milho.melt(id_vars='primeiro_ano', value_vars=colunas_numericas,
                  var_name='Métrica', value_name='Valor')

# Adicionar versão normalizada das variáveis
df_norm = df_milho.copy()
for coluna in colunas_numericas:
    df_norm[coluna] = (df_milho[coluna] - df_milho[coluna].min()) / (df_milho[coluna].max() - df_milho[coluna].min())

df_melt_norm = df_norm.melt(id_vars='primeiro_ano', value_vars=colunas_numericas,
                  var_name='Métrica', value_name='Valor Normalizado')

sns.boxplot(x='Métrica', y='Valor Normalizado', data=df_melt_norm)
plt.title('Distribuição normalizada das variáveis agrícolas')
plt.grid(True, linestyle='--', alpha=0.7)
plt.ylabel('Valores normalizados (0-1)')
plt.show()

# Relações entre variáveis (dispersão)
print("\n Dispersão - Relações entre variáveis:")
plt.figure(figsize=(16, 5))

# Gráfico 1: Área Plantada vs Produção
plt.subplot(1, 3, 1)
plt.scatter(df_milho['area_plantada'], df_milho['producao'], alpha=0.6, color='green')
plt.title('Área Plantada vs Produção')
plt.xlabel('Área Plantada (mil ha)')
plt.ylabel('Produção (mil t)')
plt.grid(True, linestyle='--', alpha=0.7)

# Gráfico 2: Área Plantada vs Produtividade
plt.subplot(1, 3, 2)
plt.scatter(df_milho['area_plantada'], df_milho['produtividade'], alpha=0.6, color='blue')
plt.title('Área Plantada vs Produtividade')
plt.xlabel('Área Plantada (mil ha)')
plt.ylabel('Produtividade (t/ha)')
plt.grid(True, linestyle='--', alpha=0.7)

# Gráfico 3: Produtividade vs Produção
plt.subplot(1, 3, 3)
plt.scatter(df_milho['produtividade'], df_milho['producao'], alpha=0.6, color='orange')
plt.title('Produtividade vs Produção')
plt.xlabel('Produtividade (t/ha)')
plt.ylabel('Produção (mil t)')
plt.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# Boxplots agrupados por safra
print("\n Boxplots - Evolução por safra:")
plt.figure(figsize=(15, 12))

plt.subplot(3, 1, 1)
sns.boxplot(x='primeiro_ano', y='area_plantada', data=df_milho)
plt.title('Área Plantada por Safra')
plt.grid(True, linestyle='--', alpha=0.7)
plt.xticks(rotation=45)

plt.subplot(3, 1, 2)
sns.boxplot(x='primeiro_ano', y='producao', data=df_milho)
plt.title('Produção por Safra')
plt.grid(True, linestyle='--', alpha=0.7)
plt.xticks(rotation=45)

plt.subplot(3, 1, 3)
sns.boxplot(x='primeiro_ano', y='produtividade', data=df_milho)
plt.title('Produtividade por Safra')
plt.grid(True, linestyle='--', alpha=0.7)
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Matriz de correlação
print("\n Matriz de correlação entre as métricas:")
plt.figure(figsize=(10, 8))
corr = df_milho[colunas_numericas].corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, annot=True, cmap='coolwarm', mask=mask, vmin=-1, vmax=1)
plt.title('Matriz de correlação entre as métricas')
plt.show()

# Visualizações interativas com Altair
print("\n Visualização interativa de séries temporais:")

# Criar gráfico interativo para cada métrica
charts = []
for coluna in colunas_numericas:
    chart = alt.Chart(df_milho).mark_line().encode(
        x='primeiro_ano',
        y=alt.Y(coluna, title=coluna),
        tooltip=['primeiro_ano', coluna]
    ).properties(
        title=f'Série Temporal - {coluna}',
        width=300,
        height=200
    )
    charts.append(chart)

# Exibir gráficos em grid
display(alt.vconcat(*charts))

## 5. Exportação/Salvamento dos Dados Processados
print("\n5. Exportação dos Dados Processados")
df_milho.to_csv('milho.csv', index=False)
print("\n Dados filtrados salvos como 'milho.csv'")

"""# Etapa 2 – Extração de Informações Relevantes:

*   Definir variáveis-chave a serem utilizadas no modelo, a partir da análise
exploratória dos dados.
*   Analisar a relação entre NDVI e produtividade agrícola, identificando os períodos críticos de crescimento da cultura.
*   Aplicar técnicas de segmentação para destacar áreas específicas de cultivo na imagem de satélite.

Montando o dataset unico

Definimos utilizar as variaveis abaixo pois apresentaram melhor... TERMINAR DE PREENCHER
"""

print (df_satveg.head())
#print (df_climatotal.head())

# Padronização dos nomes das colunas
df_satveg.columns = df_satveg.columns.str.strip()
df_climatotal.columns = df_climatotal.columns.str.strip()
df_milho.columns = df_milho.columns.str.strip()

# Gerar NDVI médio anual
# Calcular NDVI médio anual
ndvi_medio_ano = (
    df_satveg.groupby('ano')['NDVI_media_mensal']
    .mean()
    .reset_index()
    .rename(columns={'NDVI_media_mensal': 'ndvi_medio_ano'})
)
print(ndvi_medio_ano.head())

print(df_climatotal.head())
print(df_climatotal.dtypes)

# Garantir tipos no clima e milho
# Tratando NA ou NaN antes
df_climatotal['ANO'] = pd.to_numeric(df_climatotal['ANO'], errors='coerce').astype('Int64')
df_milho['primeiro_ano'] = df_milho['primeiro_ano'].astype(int)
ndvi_medio_ano['ano'] = ndvi_medio_ano['ano'].astype(int)

print(df_climatotal.head())
print(df_milho.head())
print(ndvi_medio_ano.head())

# Filtrar anos a partir de 2015
anos_validos = set(
    df_milho[df_milho['primeiro_ano'] >= 2015]['primeiro_ano']
) & set(ndvi_medio_ano[ndvi_medio_ano['ano'] >= 2015]['ano']) & set(df_climatotal[df_climatotal['ANO'] >= 2015]['ANO'])

print(anos_validos)

milho_2015 = df_milho[df_milho['primeiro_ano'] >= 2015].copy()
ndvi_2015 = ndvi_medio_ano[ndvi_medio_ano['ano'] >= 2015].copy()
clima_2015 = df_climatotal[df_climatotal['ANO'] >= 2015].copy()

# Merges
# Merge Milho + NDVI anual
base = milho_2015.merge(ndvi_2015, left_on='primeiro_ano', right_on='ano', how='left')
#print(base.head())

#Merge resultado e clima
base = base.merge(clima_2015, left_on='primeiro_ano', right_on='ANO', how='left')
print(base.head())

# Interpolação de clima

base = base.sort_values('primeiro_ano').reset_index(drop=True)

# # Criar as colunas 'temp_media_ciclo' e 'rad_media_ciclo' se não existirem, preenchendo com NaN

if 'temp_media_ciclo' not in base.columns:
    base['temp_media_ciclo'] = np.nan
if 'rad_media_ciclo' not in base.columns:
    base['rad_media_ciclo'] = np.nan

# Flags de imputação para analisar depois quais foram preenchidos
base['imputado_temp'] = base['temp_media_ciclo'].isnull()
base['imputado_rad'] = base['rad_media_ciclo'].isnull()

# Interpolação linear
base['temp_media_ciclo'] = base['temp_media_ciclo'].interpolate(method='linear', limit_direction='both')
base['rad_media_ciclo']  = base['rad_media_ciclo'].interpolate(method='linear', limit_direction='both')

# Validando resultados
print(base[['primeiro_ano', 'produtividade', 'ndvi_medio_ano',
            'temp_media_ciclo', 'rad_media_ciclo',
            'imputado_temp', 'imputado_rad']])

# Exporta dados processados
print("\n5. Exportação dos dados processados")
base.to_csv('base.csv', index=False)
print("- Dados processados salvos como 'base.csv'")

"""Analisar a relação entre NDVI e produtividade agrícola, identificando os períodos críticos de crescimento da cultura.

"""

# Biblioteca necessárias
import pandas as pd
import matplotlib.pyplot as plt

# Upload do arquivo
base = pd.read_csv('base.csv')
print(base.head())

# Avaliar relação NDVI X produtividade (correlação)
plt.figure(figsize=(10, 6))
plt.scatter(base['ndvi_medio_ano'], base['produtividade'])
plt.xlabel('NDVI médio anual')
plt.ylabel('Produtividade (kg/ha)')
plt.title('Relação entre NDVI médio anual e produtividade')
plt.show()

cor = base['ndvi_medio_ano'].corr(base['produtividade'])
print(f'Correlação entre NDVI médio anual e produtividade: {cor:.2f}')

# Análise de periodos criticos  (com NDVI mensal por ano)

# Criar tabela de NDVI mensal (pivoteamento)
tabela = df_satveg[df_satveg['ano'] >= 2015].pivot_table(index='ano', columns='mes', values='NDVI_media_mensal', aggfunc='mean')
tabela.columns = [f'ndvi_mes_{m:02d}' for m in tabela.columns]
tabela.reset_index(inplace=True)

# Juntar com produtividade
prod = base[['primeiro_ano', 'produtividade']].rename(columns={'primeiro_ano':'ano'})
analise = prod.merge(tabela, on='ano', how='left')

# Calcular correlação de NDVI de cada mês X produtividade
correlacoes = {}
for col in analise.columns:
    if col.startswith('ndvi_mes_'):
        correlacoes[col] = analise[col].corr(analise['produtividade'])

correlacoes = pd.Series(correlacoes).sort_values(ascending=False)
print('- Correlação de NDVI mensal com produtividade -')
print(correlacoes)

# Visualizar
correlacoes.plot(kind='bar', figsize=(10,4), title='Correlação NDVI mensal vs. produtividade')
plt.ylabel('Correlação')
plt.show()

plt.show()
print("\nJan apresentou a maior correlação.\n")
print("Sugerindo que esse período é especialmente crítico para o sucesso produtivo da cultura analisada.")

# Janelas móveis de NDVI médio
for mes_inicio in range(1, 11):
    colunas = [f'ndvi_mes_{mes_inicio:02d}', f'ndvi_mes_{mes_inicio+1:02d}', f'ndvi_mes_{mes_inicio+2:02d}']
    if all(col in analise.columns for col in colunas):
        analise[f'janela_{mes_inicio:02d}_{mes_inicio+2:02d}'] = analise[colunas].mean(axis=1)

# Repetir a análise de correlação para as novas colunas
cor_janelas = {col: analise[col].corr(analise['produtividade'])
               for col in analise.columns if col.startswith('janela_')}
cor_janelas = pd.Series(cor_janelas).sort_values(ascending=False)
print('- Correlação das janelas de NDVI com a produtividade -')
print(cor_janelas)

cor_janelas.plot(kind='bar', figsize=(10,4), title='Correlações NDVI janelas vs. produtividade')
plt.ylabel('Correlação')
plt.show()

plt.show()
print("\nJan/Mar apresentou a maior correlação.\n")
print("Conclusão: Vigor da vegetação no início do ciclo é determinante para produtividade.")

"""Aplicar técnicas de segmentação para destacar áreas específicas de cultivo na imagem de satélite."""

# Instalar pacotes necessários
!pip install earthengine-api geemap

# Importar bibliotecas
import ee
import geemap
import os
from google.colab import files

# Autenticar e inicializar o Earth Engine
ee.Authenticate()
ee.Initialize(project="pipoca-452815")

# Definir área de interesse: Sorriso-MT
sorriso = ee.Geometry.Point(-55.71, -12.55).buffer(30000)  # 30km de buffer

# Definir período de uma safra completa
start_date = '2023-10-01'  # Início do plantio da soja
end_date = '2024-04-30'    # Após colheita da soja e possível safrinha

# Obter coleção Sentinel-2 com bandas específicas
s2 = ee.ImageCollection('COPERNICUS/S2_SR') \
    .filterDate(start_date, end_date) \
# Obter coleção Sentinel-2 com bandas específicas
s2 = ee.ImageCollection('COPERNICUS/S2_SR') \
    .filterDate(start_date, end_date) \
    .filterBounds(sorriso) \
    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \
    .select(['B2', 'B3', 'B4', 'B8']) \
    .median()  # Mediana para remover nuvens

# Calcular NDVI
ndvi = s2.normalizedDifference(['B8', 'B4']).rename('NDVI')
s2 = s2.addBands(ndvi)

# Visualizar no mapa
Map = geemap.Map()
Map.centerObject(sorriso, 10)
Map.addLayer(s2, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000}, 'RGB Natural')
Map.addLayer(ndvi, {'min': 0, 'max': 0.8, 'palette': ['white', 'yellow', 'green']}, 'NDVI')
Map

# Montar o Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Definir pasta no Drive
drive_folder = "EarthEngine_Exports"  # Nome da pasta que será criada no Drive

# Função para exportar para o Drive
def export_to_drive(image, name, region, scale=30):
    task = ee.batch.Export.image.toDrive(
        image=image,
        description=name,
        folder=drive_folder,
        fileNamePrefix=name,
        region=region.getInfo()['coordinates'],
        scale=scale,
        maxPixels=1e9
    )
    task.start()
    print(f"Exportando {name} para o Google Drive...")
    return task

# Exportar imagens para o Drive
export_to_drive(
    s2.select(['B4', 'B3', 'B2']).divide(3000).multiply(255).byte(),
    "sorriso_rgb",
    sorriso,
    scale=30  # Resolução reduzida para tornar o arquivo menor
)

export_to_drive(
    ndvi,
    "sorriso_ndvi",
    sorriso,
    scale=30
)

print("\nImagens estão sendo exportadas para seu Google Drive.")
print("Este processo pode levar alguns minutos. As imagens estarão na pasta", drive_folder)
print("Você pode verificar o progresso em: https://code.earthengine.google.com/tasks")
print("Quando as tarefas estiverem concluídas, você poderá acessar os arquivos no seu Google Drive")

"""Upload das imagens do Drive e aplicação das técnicas de segmentação"""

# Montar o Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Importar bibliotecas necessárias
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from skimage import filters
from skimage.segmentation import watershed
from skimage.color import label2rgb
import rasterio

# Funções para carregar imagens
def load_rgb_image(path):
    with rasterio.open(path) as src:
        profile = src.profile
        # Ler as três bandas (R, G, B)
        rgb_raw = src.read()
        # Reorganizar para formato de visualização (H, W, C)
        rgb = np.dstack((rgb_raw[0], rgb_raw[1], rgb_raw[2]))
        # Normalizar para visualização
        rgb = np.clip(rgb/255.0, 0, 1)
        return rgb_raw, rgb, profile

def load_ndvi_image(path):
    with rasterio.open(path) as src:
        profile = src.profile
        # NDVI geralmente é um único canal
        ndvi_raw = src.read(1)
        return ndvi_raw, profile

# Configure os caminhos para os arquivos no Drive
# Ajuste esses caminhos para onde seus arquivos estão armazenados
rgb_path = '/content/drive/MyDrive/Pipoca/sorriso_rgb.tif'
ndvi_path = '/content/drive/MyDrive/Pipoca/sorriso_ndvi.tif'

# Verificar se os arquivos existem antes de carregar
print("Carregando imagens...")
if os.path.exists(rgb_path) and os.path.exists(ndvi_path):
    rgb_raw, rgb, rgb_profile = load_rgb_image(rgb_path)
    ndvi_raw, ndvi_profile = load_ndvi_image(ndvi_path)
    print("Imagens carregadas com sucesso!")
else:
    print(f"Erro: Arquivo(s) não encontrado(s).")
    print(f"Verifique se os caminhos estão corretos:")
    print(f"RGB: {rgb_path}")
    print(f"NDVI: {ndvi_path}")

    # Listar os arquivos disponíveis na pasta EarthEngine_Exports
    export_dir = '/content/drive/MyDrive/EarthEngine_Exports/'
    if os.path.exists(export_dir):
        print("\nArquivos disponíveis na pasta EarthEngine_Exports:")
        for file in os.listdir(export_dir):
            print(f"- {file}")
    else:
        print("\nPasta EarthEngine_Exports não encontrada.")

# Visualizar as imagens
plt.figure(figsize=(15, 10))
plt.subplot(1, 2, 1)
plt.title('Imagem RGB')
plt.imshow(rgb)
plt.axis('off')
plt.subplot(1, 2, 2)
plt.title('Imagem NDVI')
plt.imshow(ndvi_raw, cmap='RdYlGn')
plt.colorbar(label='NDVI')
plt.axis('off')
plt.tight_layout()
plt.show()

# Imprimir algumas informações sobre as imagens
print(f"Dimensões da imagem RGB: {rgb.shape}")
print(f"Dimensões da imagem NDVI: {ndvi_raw.shape}")
print(f"Valor mínimo NDVI: {np.min(ndvi_raw)}, Valor máximo NDVI: {np.max(ndvi_raw)}")

# Segmentação baseada em limiar de NDVI
print("\n1. Aplicando segmentação por limiar de NDVI...")

# Definir limiares para diferentes classes de vegetação
ndvi_thresholds = {
    'água': -1.0,
    'solo_nu': 0.2,
    'veg_esparsa': 0.4,
    'veg_densa': 0.6
}
# Criar mapa de segmentação baseado em NDVI
ndvi_segments = np.zeros_like(ndvi_raw, dtype=np.uint8)
ndvi_segments[(ndvi_raw > ndvi_thresholds['água']) & (ndvi_raw <= ndvi_thresholds['solo_nu'])] = 1 # Solo nu
ndvi_segments[(ndvi_raw > ndvi_thresholds['solo_nu']) & (ndvi_raw <= ndvi_thresholds['veg_esparsa'])] = 2 # Vegetação esparsa
ndvi_segments[(ndvi_raw > ndvi_thresholds['veg_esparsa']) & (ndvi_raw <= ndvi_thresholds['veg_densa'])] = 3 # Vegetação moderada
ndvi_segments[ndvi_raw > ndvi_thresholds['veg_densa']] = 4 # Vegetação densa

# Visualizar segmentação por NDVI
plt.figure(figsize=(15, 10))
plt.subplot(1, 2, 1)
plt.title('Imagem NDVI Original')
plt.imshow(ndvi_raw, cmap='RdYlGn')
plt.colorbar(label='NDVI')
plt.axis('off')
plt.subplot(1, 2, 2)
plt.title('Segmentação por Limiares de NDVI')
cmap = plt.cm.get_cmap('viridis', 5) # 5 classes (incluindo 0)
im = plt.imshow(ndvi_segments, cmap=cmap)
cbar = plt.colorbar(im, ticks=[0, 1, 2, 3, 4])
cbar.set_ticklabels(['Sem dados', 'Solo nu', 'Vegetação esparsa', 'Vegetação moderada', 'Vegetação densa'])
plt.axis('off')
plt.tight_layout()
plt.show()

# Métodos de segmentação (K-means e Watershed)
from sklearn.cluster import KMeans
from skimage import filters
from skimage.segmentation import watershed
from skimage.color import label2rgb

# Segmentação por clustering (K-MEANS)
print("\n2. Aplicando segmentação por K-means...")
# Preparar dados para clustering
# Reorganizar RGB para forma adequada
rgb_reshaped = rgb.reshape(rgb.shape[0], rgb.shape[1], 3)

# Empilhar RGB e NDVI
X = np.dstack([rgb_reshaped, ndvi_raw.reshape(ndvi_raw.shape[0], ndvi_raw.shape[1], 1)])

# Reshape para clustering
X_reshaped = X.reshape((-1, 4)) # Reshape para (n_pixels, n_features)

# Remover NaN ou valores inválidos para o K-means
mask = ~np.isnan(X_reshaped).any(axis=1)
X_valid = X_reshaped[mask]

# Aplicar K-means (definir k=5 para diferentes tipos de cobertura do solo)
k = 5 # Número de clusters (ajuste conforme necessário)
print(f"Aplicando K-means com {k} clusters em {X_valid.shape[0]} pixels válidos...")
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_valid)

# Reconstruir imagem de segmentação
segmentation = np.zeros(X_reshaped.shape[0], dtype=np.uint8)
segmentation[mask] = labels + 1 # +1 para evitar 0
segmentation = segmentation.reshape(X.shape[:2])

# Visualizar segmentação por K-means
plt.figure(figsize=(15, 10))
plt.subplot(1, 2, 1)
plt.title('Imagem RGB Original')
plt.imshow(rgb)
plt.axis('off')
plt.subplot(1, 2, 2)
plt.title('Segmentação K-means (5 clusters)')
cmap = plt.cm.get_cmap('tab10', k+1) # +1 para incluir 0 (sem dados)
im = plt.imshow(segmentation, cmap=cmap)
plt.colorbar(im, ticks=range(k+1))
plt.axis('off')
plt.tight_layout()
plt.show()

# Segmenta~]ap de Watershed (baseada em gradientes)
print("\n3. Aplicando segmentação Watershed...")
# Criar uma imagem de gradiente a partir do NDVI para watershed
edges = filters.sobel(ndvi_raw)

# Usar watershed para segmentar
markers = np.zeros_like(ndvi_raw, dtype=np.int32)
# Definir marcadores baseados em NDVI
markers[ndvi_raw < 0.2] = 1 # Baixo NDVI (solo nu, água)
markers[ndvi_raw > 0.6] = 2 # Alto NDVI (vegetação densa)

# Aplicar watershed
watershed_segments = watershed(edges, markers)

# Visualizar segmentação watershed
plt.figure(figsize=(15, 10))
plt.subplot(1, 2, 1)
plt.title('Imagem de Bordas (Gradientes)')
plt.imshow(edges, cmap='gray')
plt.axis('off')
plt.subplot(1, 2, 2)
plt.title('Segmentação Watershed')
overlay = label2rgb(watershed_segments, image=rgb, bg_label=0)
plt.imshow(overlay)
plt.axis('off')
plt.tight_layout()
plt.show()

"""# Etapa 3 – Construção do Modelo de IA 🤖

*   Selecionar um modelo de aprendizado de máquina ou redes neurais para previsão da produtividade.
*   Treinar o modelo utilizando os dados históricos e validar sua capacidade preditiva.
*   Ajustar hiperparâmetros e otimizar o desempenho do modelo.





"""

# Importar bibliotecas necessárias para modelagem
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns

# Extrair características das imagens segmentadas
def extract_features(rgb_image, ndvi_image, ndvi_segments, segments_kmeans, segments_watershed):
    # Criar dataframe para armazenar as características
    features = pd.DataFrame()

    # Estatísticas básicas do NDVI
    # Para cada segmento do K-means, calcular estatísticas de NDVI
    for segment_id in np.unique(segments_kmeans):
        if segment_id == 0:
            continue  # Ignorar background

        mask = segments_kmeans == segment_id
        segment_ndvi = ndvi_image[mask]

        if len(segment_ndvi) > 0:
            features[f'ndvi_mean_segment_{segment_id}'] = [np.mean(segment_ndvi)]
            features[f'ndvi_std_segment_{segment_id}'] = [np.std(segment_ndvi)]
            features[f'ndvi_min_segment_{segment_id}'] = [np.min(segment_ndvi)]
            features[f'ndvi_max_segment_{segment_id}'] = [np.max(segment_ndvi)]
            features[f'area_segment_{segment_id}'] = [np.sum(mask)]

    # Características globais da imagem
    features['ndvi_mean_global'] = [np.mean(ndvi_image)]
    features['ndvi_std_global'] = [np.std(ndvi_image)]

    # Proporção de cada classe de vegetação (baseada na segmentação por limiar de NDVI)
    total_pixels = ndvi_image.size
    for class_id in range(1, 5):  # Classes 1 a 4 (solo nu, veg esparsa, veg moderada, veg densa)
        class_count = np.sum(ndvi_segments == class_id)
        features[f'proportion_class_{class_id}'] = [class_count / total_pixels]

    # Características de textura (simplificadas)
    # Variância local como medida simples de textura
    from scipy.ndimage import generic_filter
    texture = generic_filter(ndvi_image, np.var, size=5)
    features['texture_mean'] = [np.mean(texture)]
    features['texture_std'] = [np.std(texture)]

    return features

# Aplicar a extração de características
print("Extraindo características das imagens...")
features_df = extract_features(rgb, ndvi_raw, ndvi_segments, segmentation, watershed_segments)
print(f"Características extraídas: {features_df.shape[1]}")
print(features_df.head())

# Preparação dos dados para treinamento (Dados CONAB)
# Carregando os dados
import pandas as pd

# Carregar dados de produtividade
produtividade = pd.read_csv('milho.csv')

print("Dados de produtividade carregados:")
print(f"Número de registros: {len(produtividade)}")
print(produtividade.head())

# Extrair características para cada imagem/parcela
all_features = pd.DataFrame()

# Para cada imagem/parcela no seu conjunto de dados
# Iterar sobre cada ano no conjunto de dados de produtividade
for ano in produtividade['primeiro_ano'].unique():
    # Filtrar os dados de produtividade para o ano atual
    data_ano = produtividade[produtividade['primeiro_ano'] == ano]

    # Carregar imagem e dados NDVI correspondentes
    # (ajuste os caminhos conforme necessário)
    rgb_image, _, _ = load_rgb_image(rgb_path)  # Updated to use load_rgb_image
    ndvi_image, _ = load_ndvi_image(ndvi_path)  # Updated to use load_ndvi_image
    segments_kmeans = segmentation # Updated to use segmentation (replace with load segmentation if you have a different function)
    segments_watershed = watershed_segments # Updated to use watershed_segments (replace with load segmentation if you have a different function)
    ndvi_segments = ndvi_segments # Updated to use ndvi_segments (replace with load segmentation if you have a different function)

    # Extrair as características para o ano atual
    features = extract_features(rgb_image, ndvi_image, ndvi_segments, segments_kmeans, segments_watershed)

    # Adicionar o ano como identificador para junção posterior
    features['primeiro_ano'] = ano

    # Adicionar ao conjunto completo
    all_features = pd.concat([all_features, features], ignore_index=True)

# Juntar características com dados de produtividade
dataset = pd.merge(all_features, produtividade,
                  on=['primeiro_ano'],
                  how='inner')

print("\nConjunto de dados combinado:")
print(f"Número de registros: {len(dataset)}")
print(f"Número de características: {dataset.shape[1]}")
print(dataset.head())

# Verificar se há valores ausentes
missing_values = dataset.isnull().sum()
print("\nValores ausentes por coluna:")
print(missing_values[missing_values > 0])

# Tratar valores ausentes (se necessário)
dataset = dataset.dropna()  # Ou use dataset.fillna() para preencher

# Dividir em características (X) e alvo (y)
X = dataset.drop(['productivity', 'parcel_id', 'date', 'image_path', 'ndvi_path',
                 'segmentation_path', 'watershed_path', 'ndvi_segments_path'], axis=1)
y = dataset['productivity']

# Dividir em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("\nDados divididos em conjuntos de treinamento e teste:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Normalizar os dados
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nDados normalizados e prontos para treinamento.")
